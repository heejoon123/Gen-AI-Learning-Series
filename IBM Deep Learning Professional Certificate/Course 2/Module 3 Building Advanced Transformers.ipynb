{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of a Transformer architecture.\n",
    "- Implement a multi-head self-attention mechanism from scratch.\n",
    "- Train and evaluate a Transformer for time series prediction.\n",
    "- Handle preprocessing and scaling for time series data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "The Transformer architecture was introduced in the paper *\"Attention Is All You Need\"*. It revolutionized natural language processing by using attention mechanisms instead of recurrence.\n",
    "\n",
    "### Key Components:\n",
    "- **Input Embedding:** Converts input tokens (or time steps) into vectors.\n",
    "- **Positional Encoding:** Injects information about the position of input tokens.\n",
    "- **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    "- **Feedforward Layers:** Process the attended information.\n",
    "- **Layer Normalization & Residual Connections:** Stabilize and speed up training.\n",
    "\n",
    "> Transformers are now widely used not only in NLP but also in time series forecasting, image recognition, and more.\n",
    "\n",
    "**Next:** You will implement parts of this architecture step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m188.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 pyarrow-20.0.0 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m152.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.5 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.3.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 00:51:13.878691: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-14 00:51:13.879930: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-14 00:51:13.884440: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-14 00:51:13.896370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752454273.916325     300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752454273.922057     300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752454273.938759     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752454273.938784     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752454273.938786     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752454273.938787     300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-14 00:51:13.944163: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 00:51:51.857219: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 13.1941 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2554 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1716 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1643 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2103 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1939 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1111\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1232 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1198 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1031 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1319 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1050 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1033 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0783\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0727 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.1057 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0591 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0643 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0405 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0459 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ee3936112e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 309ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlyRJREFUeJzs3Xd0FFUbwOHf7qZ3EkiDBELvvQXphN5BBUQBpUsRURFUEBREiij4KVgBFaQoXaT33nuH0HtJQgipO98fMZtMdtN303ifczhk7ty5c2c3yb65VaMoioIQQgghRD6lzekKCCGEEEJYkgQ7QgghhMjXJNgRQgghRL4mwY4QQggh8jUJdoQQQgiRr0mwI4QQQoh8TYIdIYQQQuRrEuwIIYQQIl+TYEcIIYQQ+ZoEOyJXKlasGH369DEcb9u2DY1Gw7Zt28x2D41Gw/jx481WnhAAU6dOpWzZsuj1+hy5/9WrV9FoNEyfPj1H7p9Z48ePR6PRmLXMxo0b07hxY7OWaU7z5s1Do9Fw6NChVPONHj2aOnXqZFOt8icJdoSRhB/AhH92dnaULl2aoUOHcu/evZyuXoasXbtWApokEj5Q0vqX0x8QCcFtwj9bW1u8vLxo3LgxX3zxBQ8ePMh02WfOnGH8+PFcvXrVfBX+T1hYGFOmTOHDDz9Eq0389Zr89XV0dKR8+fJMnDiRiIiITN3Lkt/bV69e5c0336REiRLY2dnh7e1Nw4YN+fTTTy1yv5xWrFgxo995pUqV4oMPPuDx48c5XT1GjBjB8ePHWbVqVU5XJc+yyukKiNzrs88+IyAggMjISHbt2sXs2bNZu3Ytp06dwsHBIVvr0rBhQ54/f46NjU2Grlu7di3fffedyQ+F58+fY2X1Yv0IdOnShZIlSxqOw8PDGTx4MJ07d6ZLly6GdC8vr5yonpHhw4dTq1Yt4uLiePDgAXv27OHTTz9lxowZLFmyhKZNm2a4zDNnzjBhwgQaN25MsWLFzFrfX3/9ldjYWHr06GF0rnnz5vTq1QuIf9137tzJ2LFjOX78OEuXLs3wvVL73s6KS5cuUatWLezt7XnrrbcoVqwYd+7c4ciRI0yZMoUJEyaY9X65RdWqVXnvvfcAiIyM5PDhw3zzzTds376dAwcO5GjdvL296dixI9OnT6dDhw45Wpe86sX6TS8ypHXr1tSsWROAfv364eHhwYwZM1i5cqXJX+YAz549w9HR0ex10Wq12NnZmbVMc5eXF1SuXJnKlSsbjh8+fMjgwYOpXLkyr7/+eorXRUZGYmNjo2qtyA4NGjTg5ZdfVqUdP36cFi1a0LVrV86cOYOPj0+21ik1c+fOpUOHDia/t0qXLq16jQcNGkR0dDTLli0jMjIy13w/fv3114SHh3Ps2DGKFi2qOnf//v0cqpXlFS5cWPX+9OvXDycnJ6ZPn87FixcpVapUDtYOXn31VV555RWuXLlC8eLFc7QueZF0Y4l0S/grOjg4GIA+ffrg5OTE5cuXadOmDc7OzvTs2RMAvV7PN998Q4UKFbCzs8PLy4uBAwfy5MkTVZmKojBx4kSKFCmCg4MDTZo04fTp00b3TmnMzv79+2nTpg0FChTA0dGRypUrM3PmTEP9vvvuO0DdjZDA1Jido0eP0rp1a1xcXHBycqJZs2bs27dPlSehm2/37t2MHDmSQoUK4ejoSOfOnY26Vw4dOkTLli0pWLAg9vb2BAQE8NZbb6X6Ordr1y7FX2aBgYGGABRg48aN1K9fHzc3N5ycnChTpgwfffRRquWnJeG1XrRoEZ988gmFCxfGwcGBsLCwFMdVJLwmybuG/v33Xxo0aICjoyPOzs60bdvW5PubEVWqVOGbb74hJCSE//3vf4b0a9eu8fbbb1OmTBns7e3x8PDglVdeUdVp3rx5vPLKKwA0adLE8D2R8H21cuVK2rZti6+vL7a2tpQoUYLPP/+cuLi4NOsVHBzMiRMnCAoKSvezeHt7o9FojFoYly5dSo0aNbC3t6dgwYK8/vrr3Lp1y3A+re/tBD/++CMlSpTA1taWWrVqcfDgwTTrdPnyZYoUKWIU6AB4enoapf377780atQIZ2dnXFxcqFWrFgsXLjSc37lzJ6+88gr+/v7Y2tri5+fHu+++y/Pnz9OsC8Aff/xheC3c3d3p3r07N27cSPFZ7e3tqV27Njt37kxX+anx9vYGUL0/J06coE+fPhQvXtzQxffWW2/x6NEjo+tv3bpF3759Dd9PAQEBDB48mOjo6BTv+eTJE2rXrk2RIkU4f/68IT3h+2rlypVZfq4XkbTsiHS7fPkyAB4eHoa02NhYWrZsSf369Zk+fbqhe2vgwIHMmzePN998k+HDhxMcHMz//vc/jh49yu7du7G2tgZg3LhxTJw4kTZt2tCmTRuOHDlCixYtUv1lkGDjxo20a9cOHx8f3nnnHby9vTl79ixr1qzhnXfeYeDAgdy+fZuNGzfy+++/p1ne6dOnadCgAS4uLowaNQpra2t++OEHGjduzPbt240GCA4bNowCBQrw6aefcvXqVb755huGDh3K4sWLgfi/glu0aEGhQoUYPXo0bm5uXL16lWXLlqVaj27dutGrVy8OHjxIrVq1DOnXrl1j3759TJs2zVDfdu3aUblyZT777DNsbW25dOkSu3fvTvNZ0+Pzzz/HxsaG999/n6ioqAx3If7+++/07t2bli1bMmXKFCIiIpg9ezb169fn6NGjWepCevnll+nbty8bNmxg0qRJABw8eJA9e/bQvXt3ihQpwtWrV5k9ezaNGzfmzJkzODg40LBhQ4YPH86sWbP46KOPKFeuHIDh/3nz5uHk5MTIkSNxcnJiy5YtjBs3jrCwMMPrnpI9e/YAUL16dZPnIyMjefjwIRDfArp7927mz5/Pa6+9pvowTfi5qVWrFpMnT+bevXvMnDmT3bt3c/ToUdzc3NL1vb1w4UKePn3KwIED0Wg0TJ06lS5dunDlyhXDz58pRYsWZdOmTWzZsiXNbsJ58+bx1ltvUaFCBcaMGYObmxtHjx5l3bp1vPbaa0B84BYREcHgwYPx8PDgwIEDfPvtt9y8eTPN7rtJkyYxduxYXn31Vfr168eDBw/49ttvadiwoeG1APjll18YOHAg9erVY8SIEVy5coUOHTrg7u6On59fqvdIEBMTY3h/IiMjOXr0KDNmzKBhw4YEBAQY8m3cuJErV67w5ptv4u3tzenTp/nxxx85ffo0+/btMwSdt2/fpnbt2oSEhDBgwADKli3LrVu3+Ouvv4iIiDD58/Tw4UOaN2/O48eP2b59OyVKlDCcc3V1pUSJEuzevZt33303Xc8kklCESGbu3LkKoGzatEl58OCBcuPGDWXRokWKh4eHYm9vr9y8eVNRFEXp3bu3AiijR49WXb9z504FUBYsWKBKX7dunSr9/v37io2NjdK2bVtFr9cb8n300UcKoPTu3duQtnXrVgVQtm7dqiiKosTGxioBAQFK0aJFlSdPnqjuk7SsIUOGKCl9mwPKp59+ajju1KmTYmNjo1y+fNmQdvv2bcXZ2Vlp2LCh0esTFBSkute7776r6HQ6JSQkRFEURVm+fLkCKAcPHjR5/5SEhoYqtra2ynvvvadKnzp1qqLRaJRr164piqIoX3/9tQIoDx48yFD5ST148MDodUh4rYsXL65ERESo8n/66acmX8+E1yQ4OFhRFEV5+vSp4ubmpvTv31+V7+7du4qrq6tRenIJdVi6dGmKeapUqaIUKFDAcJy8roqiKHv37lUA5bfffjOkLV26VPW9lJSpMgYOHKg4ODgokZGRqdb5k08+UQDl6dOnRucAk/86deqkKjc6Olrx9PRUKlasqDx//tyQvmbNGgVQxo0bZ0hL6Xs7ODhYARQPDw/l8ePHhvSVK1cqgLJ69epUn+PUqVOKvb29AihVq1ZV3nnnHWXFihXKs2fPVPlCQkIUZ2dnpU6dOqq6Kor6Z9DUazp58mTV97KiGH9vXb16VdHpdMqkSZNU1548eVKxsrIypCe8ZlWrVlWioqIM+X788UcFUBo1apTq8yqKohQtWtTk+/PSSy8pDx8+VOU19Tx//vmnAig7duwwpPXq1UvRarUmf/4TXp+En5uDBw8qd+7cUSpUqKAUL15cuXr1qsl6tmjRQilXrlyazyOMSTeWSFFQUBCFChXCz8+P7t274+TkxPLlyylcuLAq3+DBg1XHS5cuxdXVlebNm/Pw4UPDvxo1auDk5MTWrVsB2LRpE9HR0QwbNkzVBD9ixIg063b06FGCg4MZMWKE4a+7BJmZvhoXF8eGDRvo1KmTqgvJx8eH1157jV27dhEWFqa6ZsCAAap7NWjQgLi4OK5duwZgqNeaNWuIiYlJd11cXFxo3bo1S5YsQVEUQ/rixYupW7cu/v7+qvJXrlxpkWnOvXv3xt7ePlPXbty4kZCQEHr06KH6HtDpdNSpU8fwPZAVTk5OPH361HCctK4xMTE8evSIkiVL4ubmxpEjR9JVZtIynj59ysOHD2nQoAERERGcO3cu1WsfPXqElZUVTk5OJs937NiRjRs3snHjRlauXMmYMWMMLSAJ7/OhQ4e4f/8+b7/9tmoMT9u2bSlbtiz//PNPup4D4lsICxQoYDhu0KABAFeuXEn1ugoVKnDs2DFef/11rl69ysyZM+nUqRNeXl789NNPhnwbN27k6dOnjB492mi8UdKfi6Sv6bNnz3j48CH16tVDURSOHj2aYj2WLVuGXq/n1VdfVX0PeXt7U6pUKcP3UMJrNmjQIFVrSZ8+fXB1dU31WZOqU6eO4f1Zs2YNkyZN4vTp03To0EHV5Zb0eRJa6+rWrQtg+D7T6/WsWLGC9u3bq7qdTb0+ADdv3qRRo0bExMSwY8cOk12IAAUKFDC0PomMkW4skaLvvvuO0qVLY2VlhZeXF2XKlDEaoGplZUWRIkVUaRcvXiQ0NNRk/z4kDnJMCAqSD/wrVKiQ6pe0KQldahUrVkz/A6XiwYMHREREUKZMGaNz5cqVQ6/Xc+PGDSpUqGBITwg6EiTUOWFcUqNGjejatSsTJkzg66+/pnHjxnTq1InXXnsNW1vbVOvTrVs3VqxYwd69e6lXrx6XL182zA5Jmufnn3+mX79+jB49mmbNmtGlSxdefvllswwkTtp0n1EXL14ESLEbxMXFJdNlJwgPD8fZ2dlw/Pz5cyZPnszcuXO5deuWKlAMDQ1NV5mnT5/mk08+YcuWLUbBbXrLSEmRIkVU43k6dOiAh4cH77//PmvWrKF9+/aGnwlT34dly5Zl165d6b5fWt+fqSldujS///47cXFxnDlzhjVr1jB16lQGDBhAQEAAQUFB6f4ZvH79OuPGjWPVqlVG907tNb148SKKoqQ4MDihKy6l3yPW1tYZGshbsGBB1fvTtm1bypQpw8svv8zPP//MsGHDAHj8+DETJkxg0aJFRgO2E57nwYMHhIWFpfv30xtvvIGVlRVnz541jBMyRVEUs69F9KKQYEekqHbt2ib/KknK1tbW6INVr9fj6enJggULTF5TqFAhs9UxJ+l0OpPpCR+yGo2Gv/76i3379rF69WrWr1/PW2+9xVdffcW+fftSbAEAaN++PQ4ODixZsoR69eqxZMkStFqtYXAtxP+FuWPHDrZu3co///zDunXrWLx4MU2bNmXDhg0p1i+9TLXqpPSLNvkA3oSWpt9//93kL++sTvmPiYnhwoULqg+TYcOGMXfuXEaMGEFgYCCurq5oNBq6d++erpavkJAQGjVqhIuLC5999plhjZkjR47w4YcfplmGh4cHsbGxPH36VBWEpaZZs2YA7Nixg/bt26frmvRK6/szvWVUqlSJSpUqERgYSJMmTViwYEG6B2HHxcUZxqB8+OGHlC1bFkdHR27dukWfPn1SfU31ej0ajYZ///3X5LOk9vNjLknfn4Rg59VXX2XPnj188MEHVK1aFScnJ/R6Pa1atcp0C2uXLl347bffmDlzJpMnT04x35MnTyhYsGCm7vGik2BHmF2JEiXYtGkTL730UqrdIAlNtRcvXlT9BfbgwYM0//pMGLh36tSpVH/xpvevoEKFCuHg4KCa/ZDg3LlzaLXadA90TK5u3brUrVuXSZMmsXDhQnr27MmiRYvo169fitc4OjrSrl07li5dyowZM1i8eDENGjTA19dXlU+r1dKsWTOaNWvGjBkz+OKLL/j444/ZunVrhmYFpVdC60BISIiq+zDhr+sECe+Pp6enRerx119/8fz5c1q2bKlK6927N1999ZUhLTIykpCQENW1KX1PbNu2jUePHrFs2TIaNmxoSE+YfZiWsmXLGvInnd6fmtjYWCC+lQoSfybOnz9v1Cp2/vx5VfdGdv+Fn/CHz507dwD1z2DStZuSOnnyJBcuXGD+/PmGNYYgvgssLSVKlEBRFAICAihdunSK+ZL+Hkn6msXExBAcHEyVKlXSvFdKkr8/T548YfPmzUyYMIFx48YZ8iW0ZCYoVKgQLi4unDp1Kl33GTZsGCVLlmTcuHG4uroyevRok/my+jwvMhmzI8zu1VdfJS4ujs8//9zoXGxsrOHDJygoCGtra7799lvVX5tJu2pSUr16dQICAgxTkJNKWlbCmj/J8ySn0+lo0aIFK1euVE1VvnfvHgsXLqR+/foZ7np58uSJ0V/RVatWBSAqKirN67t168bt27f5+eefOX78ON26dVOdN7Wya0bKz4yED7gdO3YY0p49e8b8+fNV+Vq2bImLiwtffPGFyfFKWVkB+fjx44wYMYICBQowZMgQQ7pOpzN6vb/99lujVqeUvicSWg+SlhEdHc3333+frnoFBgYCpLn0f1KrV68GMHyA1axZE09PT+bMmaN6D//991/Onj1L27Zt03yOrNq5c6fJ92zt2rVAYhdbixYtcHZ2ZvLkyURGRqryJryGpl5TRVEMy0OkpkuXLuh0OiZMmGD0viqKYpjqXbNmTQoVKsScOXNUszjnzZuX5dcm+ftj6nnA+HeWVqulU6dOrF692uT3g6nWtbFjx/L+++8zZswYZs+ebXQ+NDSUy5cvU69evUw9y4tOWnaE2TVq1IiBAwcyefJkjh07RosWLbC2tubixYssXbqUmTNn8vLLL1OoUCHef/99Jk+eTLt27WjTpg1Hjx7l33//TbOpVqvVMnv2bNq3b0/VqlV588038fHx4dy5c5w+fZr169cDUKNGDSB+Jd6WLVui0+no3r27yTInTpxoWLfm7bffxsrKih9++IGoqCimTp2a4ddh/vz5fP/993Tu3JkSJUrw9OlTfvrpJ1xcXGjTpk2a1yesXfT++++j0+no2rWr6vxnn33Gjh07aNu2LUWLFuX+/ft8//33FClShPr162e4vunRokUL/P396du3Lx988AE6nY5ff/2VQoUKcf36dUM+FxcXZs+ezRtvvEH16tXp3r27Ic8///zDSy+9pFojJyU7d+4kMjKSuLg4Hj16xO7du1m1ahWurq4sX75c1UXWrl07fv/9d1xdXSlfvjx79+5l06ZNqqUSID4g1Ol0TJkyhdDQUGxtbWnatCn16tWjQIEC9O7dm+HDh6PRaPj999/T3e1TvHhxKlasyKZNm0yupXThwgX++OMPACIiIti3bx/z58+nZMmSvPHGG0D8OJMpU6bw5ptv0qhRI3r06GGYel6sWDHVlOOMfG9nxJQpUzh8+DBdunQxtFAdOXKE3377DXd3d8MEAhcXF77++mv69etHrVq1eO211yhQoADHjx8nIiKC+fPnU7ZsWUqUKMH777/PrVu3cHFx4e+//07XuKESJUowceJExowZw9WrV+nUqRPOzs4EBwezfPlyBgwYwPvvv4+1tTUTJ05k4MCBNG3alG7duhEcHMzcuXMzNGbn1q1bhvcnOjqa48eP88MPP1CwYEFDF5aLiwsNGzZk6tSpxMTEULhwYTZs2GCy9e+LL75gw4YNNGrUiAEDBlCuXDnu3LnD0qVL2bVrl9HECoBp06YRGhrKkCFDcHZ2Vi1yuGnTJhRFoWPHjul+JpFENs78EnlE0umQqendu7fi6OiY4vkff/xRqVGjhmJvb684OzsrlSpVUkaNGqXcvn3bkCcuLk6ZMGGC4uPjo9jb2yuNGzdWTp06pRQtWjTVqecJdu3apTRv3lxxdnZWHB0dlcqVKyvffvut4XxsbKwybNgwpVChQopGo1FNbSXZlGtFUZQjR44oLVu2VJycnBQHBwelSZMmyp49e9L1+iSv45EjR5QePXoo/v7+iq2treLp6am0a9dOOXToUGovq0rPnj0N09yT27x5s9KxY0fF19dXsbGxUXx9fZUePXooFy5cSHf5qU09T2na9+HDh5U6deooNjY2ir+/vzJjxgyjqedJy2rZsqXi6uqq2NnZKSVKlFD69OmT5muQUIeEf9bW1kqhQoWUhg0bKpMmTVLu379vdM2TJ0+UN998UylYsKDi5OSktGzZUjl37pzR95KiKMpPP/2kFC9eXNHpdKr3bPfu3UrdunUVe3t7xdfXVxk1apSyfv36FKeqJzdjxgzFycnJaHpy0mcBFJ1OpxQpUkQZMGCAcu/ePaNyFi9erFSrVk2xtbVV3N3dlZ49exqWfEiQ0vd2wtTzadOmGZVr6ns+ud27dytDhgxRKlasqLi6uirW1taKv7+/0qdPH9WyDAlWrVql1KtXT7G3t1dcXFyU2rVrK3/++afh/JkzZ5SgoCDFyclJKViwoNK/f3/l+PHjCqDMnTvXkC+lZQ3+/vtvpX79+oqjo6Pi6OiolC1bVhkyZIhy/vx5Vb7vv/9eCQgIUGxtbZWaNWsqO3bsUBo1apSpqedarVbx9PRUevTooVy6dEmV9+bNm0rnzp0VNzc3xdXVVXnllVeU27dvm3xtr127pvTq1UspVKiQYmtrqxQvXlwZMmSIYYq8qd8lcXFxSo8ePRQrKytlxYoVhvRu3bop9evXT/NZhGkaRcnAaDUhhBApCg0NpXjx4kydOpW+ffvmdHVEPnH37l0CAgJYtGiRtOxkkozZEUIIM3F1dWXUqFFMmzbNImsfiRfTN998Q6VKlSTQyQJp2RFCCCFEviYtO0IIIYTI1yTYEUIIIUS+JsGOEEIIIfI1CXaEEEIIka/JooLE78Fy+/ZtnJ2dZZM1IYQQIo9QFIWnT5/i6+ub6gbIEuwAt2/fzvS+R0IIIYTIWTdu3KBIkSIpnpdgBww7FN+4cSPD+x8JIYQQImeEhYXh5+dn+BxPiQQ7JO4e7OLiIsGOEEIIkcekNQRFBigLIYQQIl+TYEcIIYQQ+ZoEO0IIIYTI12TMTgbExcURExOT09UQFmZtbY1Op8vpagghhDATCXbSQVEU7t69S0hISE5XRWQTNzc3vL29Zd0lIYTIByTYSYeEQMfT0xMHBwf5AMzHFEUhIiKC+/fvA+Dj45PDNRJCCJFVEuykIS4uzhDoeHh45HR1RDawt7cH4P79+3h6ekqXlhBC5HEyQDkNCWN0HBwccrgmIjslvN8yRksIIfI+CXbSSbquXizyfgshRP4hwY4QQggh8jUJdoQQQgiRr0mwkw9pNJpU/40fPz7b6tK4cWPDfW1tbSlcuDDt27dn2bJlGS5r/PjxVK1a1fyVFEIIka9JsJMP3blzx/Dvm2++wcXFRZX2/vvvG/IqikJsbKxF69O/f3/u3LnD5cuX+fvvvylfvjzdu3dnwIABFr2vEEIIy3oeHYeiKDldjTRJsJMPeXt7G/65urqi0WgMx+fOncPZ2Zl///2XGjVqYGtry65du+jTpw+dOnVSlTNixAgaN25sONbr9UyePJmAgADs7e2pUqUKf/31V5r1cXBwwNvbmyJFilC3bl2mTJnCDz/8wE8//cSmTZsM+T788ENKly6Ng4MDxYsXZ+zYsYbZUPPmzWPChAkcP37c0FI0b948AGbMmEGlSpVwdHTEz8+Pt99+m/Dw8Cy/jkIIIVJ27dEzyo1bx/BFx3K6KmmSdXYySFEUnsfE5ci97a11ZpslNHr0aKZPn07x4sUpUKBAuq6ZPHkyf/zxB3PmzKFUqVLs2LGD119/nUKFCtGoUaMM3b9379689957LFu2jKCgIACcnZ2ZN28evr6+nDx5kv79++Ps7MyoUaPo1q0bp06dYt26dYYAydXVFQCtVsusWbMICAjgypUrvP3224waNYrvv/8+Q3USQgiRfvP2XAVg9fHbfNujWs5WJg0S7GTQ85g4yo9bnyP3PvNZSxxszPOWffbZZzRv3jzd+aOiovjiiy/YtGkTgYGBABQvXpxdu3bxww8/ZDjY0Wq1lC5dmqtXrxrSPvnkE8PXxYoV4/3332fRokWMGjUKe3t7nJycsLKywtvbW1XWiBEjVNdNnDiRQYMGSbAjhBAWpMtDS3RIsPOCqlmzZobyX7p0iYiICKMAKTo6mmrVMhfRK4qiaqlavHgxs2bN4vLly4SHhxMbG4uLi0ua5WzatInJkydz7tw5wsLCiI2NJTIykoiICFkMUgghLESnlWAn37K31nHms5Y5dm9zcXR0VB1rtVqjQWZJVw9OGAPzzz//ULhwYVU+W1vbDN8/Li6OixcvUqtWLQD27t1Lz549mTBhAi1btsTV1ZVFixbx1VdfpVrO1atXadeuHYMHD2bSpEm4u7uza9cu+vbtS3R0tAQ7QghhIXlp8VUJdjJIo9GYrSspNylUqBCnTp1SpR07dgxra2sAypcvj62tLdevX89wl5Up8+fP58mTJ3Tt2hWAPXv2ULRoUT7++GNDnmvXrqmusbGxIS5OPV7q8OHD6PV6vvrqK7Ta+PH2S5YsyXL9hBBCpE6Xh6Y45b9PbZEpTZs2Zdq0afz2228EBgbyxx9/cOrUKUMXlbOzM++//z7vvvsuer2e+vXrExoayu7du3FxcaF3794plh0REcHdu3eJjY3l5s2bLF++nK+//prBgwfTpEkTAEqVKsX169dZtGgRtWrV4p9//mH58uWqcooVK0ZwcDDHjh2jSJEiODs7U7JkSWJiYvj2229p3749u3fvZs6cOZZ7oYQQ4gUUGRPHlnP3ealkQVzt4/8ITmvMTpxeYfuF+5T1dsHXzT47qpmiPBSXCUtq2bIlY8eOZdSoUdSqVYunT5/Sq1cvVZ7PP/+csWPHMnnyZMqVK0erVq34559/CAgISLXsn376CR8fH0qUKEGXLl04c+YMixcvVg0g7tChA++++y5Dhw6latWq7Nmzh7Fjx6rK6dq1K61ataJJkyYUKlSIP//8kypVqjBjxgymTJlCxYoVWbBgAZMnTzbfCyOEEIIv1p7l7QVH6DvvoCEtrW6sBfuv8da8Q9T7cgtHrz+xdBVTpVHywmpAFhYWFoarqyuhoaFGA2IjIyMJDg4mICAAOzu7HKqhyG7yvgshRKLy49YRER0/jODql20BmLX5IjM2XlClJdX5+90cvR4CQL0SHizsX9fs9Urt8zsp6cYSQgghRKpMNYuYmo2178ojImPiOH07zBDopJQ3O+VoN9bkyZOpVasWzs7OeHp60qlTJ86fP6/KExkZyZAhQ/Dw8MDJyYmuXbty7949VZ7r16/Ttm1bHBwc8PT05IMPPrD4FghCCCHEi0LBONrRJunGCn0ew/KjN+n+4z76zD3ItPXqz/KcnrmVo8HO9u3bGTJkCPv27WPjxo3ExMTQokULnj17Zsjz7rvvsnr1apYuXcr27du5ffs2Xbp0MZyPi4ujbdu2REdHs2fPHubPn8+8efMYN25cTjySEEIIkW9ExsRx9PoTImP0Rud2XHhg+HrEoqO8u/h4quXkpBztxlq3bp3qeN68eXh6enL48GEaNmxIaGgov/zyCwsXLqRp06YAzJ07l3LlyrFv3z7q1q3Lhg0bOHPmDJs2bcLLy4uqVavy+eef8+GHHzJ+/HhsbGxy4tGEEEKIPClOr3DzSQRFPRzp9uM+jt8IMcrz5Fk0e688MhxvPf/AKE9StlY5Ox8qV83GCg0NBcDd3R2IX0MlJibGsHcSQNmyZfH392fv3r1A/GJ0lSpVwsvLy5CnZcuWhIWFcfr0aZP3iYqKIiwsTPVPCCGEEDByyTEaTdvGsiM3TQY6AF3n7EmzHHfCaKo9wu/WX9BRs9PMtcyYXDNAWa/XM2LECF566SUqVqwIwN27d7GxscHNzU2V18vLi7t37xryJA10Es4nnDNl8uTJTJgwwcxPIIQQQuR9K4/dBuD7bZdNnr947ylXHjwzea6y5jK9rDZSS3OOotr7iSeun4JbTaBwDbPXNz1yTbAzZMgQTp06xa5duyx+rzFjxjBy5EjDcVhYGH5+fha/rxBCCJFXpDSB6n9bLxm+1hFHN9022mn3Uk93JsWy/oxtQjPH0niauY7plSuCnaFDh7JmzRp27NhBkSJFDOne3t5ER0cTEhKiat25d++eYedrb29vDhw4oCovYbZW8t2xE9ja2mZqPychhBDiRaFNYQbVP8euM1i3lqraS7TUHUq1jHei32advjZR2LA9LudGzuRosKMoCsOGDWP58uVs27bNaCXeGjVqYG1tzebNmw17KJ0/f57r168TGBgIQGBgIJMmTeL+/ft4esbHjBs3bsTFxYXy5ctn7wMJIYQQ+UTS6eJa9LTR7ud/Nt+mes1RfUmmx77CQX1ZorFWnQt9HpPCVZaXo8HOkCFDWLhwIStXrsTZ2dkwxsbV1RV7e3tcXV3p27cvI0eOxN3dHRcXF4YNG0ZgYCB168avxNiiRQvKly/PG2+8wdSpU7l79y6ffPIJQ4YMkdabbNKnTx9CQkJYsWIFAI0bN6Zq1ap88803mS7THGUIIYRIH0VRiIlTsEkya8qeSEZbLWSQ1ZoUr1sVF8j2uCps1NcgDMdU7xH2POfWv8vRYGf27NlA/AdbUnPnzqVPnz4AfP3112i1Wrp27UpUVBQtW7ZU7amk0+lYs2YNgwcPJjAwEEdHR3r37s1nn32WXY+Ra/Xp04f58+cDYG1tjb+/P7169eKjjz7Cyspyb/2yZcsMu6WnZdu2bTRp0oQnT56ouiozUoYQQois+Wj5KZYfvcn2QWU4YdsXF81zeEKKUULbqEmcUYqiZGBSt4t9zoUcOd6NlRY7Ozu+++47vvvuuxTzFC1alLVr15qzavlGq1atmDt3LlFRUaxdu5YhQ4ZgbW3NmDFjVPmio6PNtiZRwtIBOV2GEEKIdIiJ5NGhv/lRtwmvn06CiaE6+/VleTv6HR7hmqGiZ7xahWVHbtGygheVi7iZp76ZkKvW2RHmZ2tri7e3N0WLFmXw4MEEBQWxatUq+vTpQ6dOnZg0aRK+vr6UKVMGgBs3bvDqq6/i5uaGu7s7HTt25OrVq4by4uLiGDlyJG5ubnh4eDBq1CijoLVx48aMGDHCcBwVFcWHH36In58ftra2lCxZkl9++YWrV6/SpEkTAAoUKIBGozG06CUv48mTJ/Tq1YsCBQrg4OBA69atuXjxouH8vHnzcHNzY/369ZQrVw4nJydatWrFnTt3DHm2bdtG7dq1cXR0xM3NjZdeeolr166Z6ZUWQoi849HTSL5duIyI75vCJC9+tPmahrqTqjyfxbxBh6jPKRa5kG7R41INdLrXMp7RPLlLJbpUL8If/erwRmAxcz9ChuSK2Vh5iqJATETO3NvaAbK4v4i9vT2PHsWverl582ZcXFzYuHEjADExMbRs2ZLAwEB27tyJlZUVEydOpFWrVpw4cQIbGxu++uor5s2bx6+//kq5cuX46quvWL58uWGFa1N69erF3r17mTVrFlWqVCE4OJiHDx/i5+fH33//TdeuXTl//jwuLi7Y29ubLKNPnz5cvHiRVatW4eLiwocffkibNm04c+aMobsrIiKC6dOn8/vvv6PVann99dd5//33WbBgAbGxsXTq1In+/fvz559/Eh0dzYEDB3J8vxYhhMg2igLPHsKBH/DYMY1hJrKc1hdleMxQLiuFM1R060o+LDp4Q5XmYKPLQmXNS4KdjIqJgC98c+beH90Gm9QHgKVEURQ2b97M+vXrGTZsGA8ePMDR0ZGff/7Z0H31xx9/oNfr+fnnnw1BwNy5c3Fzc2Pbtm20aNGCb775hjFjxhj2J5szZw7r169P8b4XLlxgyZIlbNy40bASdvHixQ3nE7qrPD09jRaPTJAQ5OzevZt69eoBsGDBAvz8/FixYgWvvPIKEB+szZkzhxIlSgDxSxokjN0KCwsjNDSUdu3aGc6XK1cu4y+kEELkNdf2wp/dITLE9Hk7N9qGfsBFpYjRDKr00pn4wzGlqes5Qbqx8rk1a9bg5OSEnZ0drVu3plu3bowfPx6ASpUqqcbpHD9+nEuXLuHs7IyTkxNOTk64u7sTGRnJ5cuXCQ0N5c6dO9SpU8dwjZWVFTVr1kzx/seOHUOn09GoUaNMP8PZs2exsrJS3dfDw4MyZcpw9uxZQ5qDg4MhkAHw8fHh/v34FTzd3d3p06cPLVu2pH379sycOVPVxSWEEPnKtT0w3jX+39xWRoHOJb0vm+OqUTXyBxh9jdNKQKYDHTC9AOHz6Jzd/DMpadnJKGuH+BaWnLp3BjVp0oTZs2djY2ODr6+vahaWo6O6lSg8PJwaNWqwYMECo3IKFSqU8fpCit1SlpB89pZGo1GNJ5o7dy7Dhw9n3bp1LF68mE8++YSNGzcaljEQQog86/EVuLABTv0NNw+YzKL3qYZSuTtlVvoQmw0f/yU8M9cTYQkS7GSURpPprqSc4OjoSMmSJdOVt3r16ixevBhPT09cXFxM5vHx8WH//v00bNgQgNjYWA4fPkz16tVN5q9UqRJ6vZ7t27erNnRNkNCyFBeX8l8A5cqVIzY2lv379xu6sR49esT58+czvHBktWrVqFatGmPGjCEwMJCFCxdKsCOEyJv0cXDkNzj4M9w7lWK2NXF1cXvjd17/9SAEG5/fdOZehm47oUMFPl1leqNtgCUDA3nwNIoaRXPPrFrpxhIGPXv2pGDBgnTs2JGdO3cSHBzMtm3bGD58ODdv3gTgnXfe4csvv2TFihWcO3eOt99+m5CQkBTLLFasGL179+att95ixYoVhjKXLFkCxC8boNFoWLNmDQ8ePCA8PNyojFKlStGxY0f69+/Prl27OH78OK+//jqFCxemY8eO6Xq24OBgxowZw969e7l27RobNmzg4sWLMm5HCJG36PWwZmR899Rn7rBmhHGg41UJui9kcasjFItcyNCY4fGBTgr6/Zb6lg/J9Qosmur58r4utK3sk6EyLU2CHWHg4ODAjh078Pf3p0uXLpQrV46+ffsSGRlpaOl57733eOONN+jduzeBgYE4OzvTuXPnVMudPXs2L7/8Mm+//TZly5alf//+PHsWv2Nu4cKFmTBhAqNHj8bLy4uhQ4eaLGPu3LnUqFGDdu3aERgYiKIorF27Nt0LDzo4OHDu3Dm6du1K6dKlGTBgAEOGDGHgwIEZeIWEECIHPHsIm8bDz0HwWQE49Iv6vKMnlGoJ/bbAR3dg8C4o25Y4jWUWZk1rFqupwco5TaOkZ2W/fC4sLAxXV1dCQ0ONum8iIyMJDg4mICAAOzu7HKqhyG7yvgshctSd47DgFQhPo4up3xYoUsMo+eaTCOpP2WqRql39si3FRv+jSlvYrw6v/bwfgPMTW2FrlT3TzlP7/E5KxuwIIYQQuYGiwNlV8a04j6+YzlN/JAQOAceCqRY1Y8MF89cvnXLTlPMEEuwIIYQQOUkfB/u+hw2fmD7vVhReWwKeZVMsYuOZe0z65wwzu1ejip8b+hzstMmN3VgS7AghhBA5ITYKzqyMH3Ac/dT4/OC94KWecbrt/H0W7L/OpM4V+WH7FR4/i2bGq1Xo/98g477zD3HoE+OZr9kpF8Y6EuwIIYQQ2WrfHFj3oelzhWtAz7/AwfS07T5z42dV2ei0/HMyfmHUgY0SV6V/GB7FnksPM7UVzpSulfjw75NpZyR+36sxy0znzY3b8Eiwk04yjvvFIu+3EMKsYqPhwr+wcwbcOaY+V6hc/DicSq+AdfomRNx4krhH47Mo9Tplr/28n87VMra3Vb/6AXSr5Z9qsDPn9RqU9nICoEdtf+ystby7+Djj25c3uVN6biLBThqSbjKZnasBi5wVERH/iyS9U9uFEMIkfRwc/AX+/cD0+S4/QeVXVUmhz2NwsbNKtYUkOlZv8uvM+qRd2gu0tqrorTruXK0ILSt442BjxZ5LD7NcB0uSYCcNOp0ONzc3wx5LDg4OubKJTpiHoihERERw//593Nzc0Olyz669Qog85vQKWNrbOL18R2j0IXhVMDq178ojuv+4j1dqFGHaK1VSLDomLkmwE2cc7Kw9mT17/znYxIcRdrloh3NTJNhJB2/v+Gg2IeAR+Z+bm5vhfRdCiAx59gimFTd97uO7YJ1yL8GszRcBWHr4pirYWXTgOsUKJm5VdPnBM8PXpjbcjMpAa8/X3VIOqtKrmp8br9QoQlGPjO/hmB0k2EkHjUaDj48Pnp6exMTE5HR1hIVZW1tLi44QInP+eit+M86kGo+Bch2gYCnQpd41nrTj4KsN53mvRRkOXn3M6BQGA0P8AoJZ0blaEcPX9tY6nsdkfLdyjUaTaktUTpNgJwN0Op18CAohhDB26wj83gkiQxPTtNbxLTm69H/UapKM9P12yyXea1GGKw+M9wxM6vxdE9PWM2nze404czuM6Dg9m8/e5+8jN81Wdk6SYEcIIYTIii0TYcc0ddpbG8CvdoYWnflg6XF2mRjoGx2X+uzQpYczH5DUDlBPcfd1s8fXLb6brU0lH1ztrfl1t4mt0vMY2QhUCCGEyIxHl+GPrupAp0RT+OAK+NfJUKAT/PBZikFLrIkByObyU6+aqZ5vXSl+7KK3S97eI1BadoQQQoiMCN4B89snHmut4sfk1HwTijVId5Cj1ytotfF5t583PQFm58UHTFh9JstVTmpS54p8vPwUAA5pzKKqVcydDe82NLT25FUS7AghhBDpde4fWPSaOu2NFRDQIF2XR8bEce1RBCER0fSdf4hx7crzai0/xqcQ0Lzxy4EsVthY5cJuhq+tdWl38JT2cjZ7HbKbBDtCCCFEWqKfwT/vwfE/1elDDkChMukuptcvBzhw9bHheNTfJ3i1lp+5apku3q52/PhGDVzsX5xFUyXYEUIIIVISGwUzq8DTZIv0vf43FG8K2owNfU0a6CQIfZ7ykiY2Oq3JRQPT67vXqqPTwqA/jhjSnGytaFHhxVpHTAYoCyGEEKbo9fC/mupAp3hjGHMTSgalGejE6ZV0rYFTZcKGFM/ZWWftY7ptZR9aVfRhVKv41qeKhV2wz+WrHVuCBDtCCCFEcju/gs8KQMj1xLTmn8WPz7FN3xiWEYuPUX/KVv45kfmtG8IiYzOUv2v1IibTBzcqwYZ3G7JySP1M1yUvk2BHCCGESHD/LKx+BzZ/lphm4wyf3IeX3klzplVEdCzfbr7IhXtPWX38NgCzt1+yZI0NKhZ2YXyH8ni52Bqd02g0lPZyRqd9Mfd2lDE7QgghhKLAH13g8hZ1epXXoP1MsLJJVzFfbbjAL7uC+WrjBUOa7r/urg2n75qtusn9PbgeNYoWAOIfRahJsCOEEOLFFvEYvqsNzx4kplV7A5p9Ck6FMlTU4WtPjNKs/mtNGfD74SxVMzWFnBJbcxqWLsRfh2+abOF5UUmwI4QQ4sUUfj9+OvnZVer0tl9Bzb6qLqubTyI4cj2EtpV8Uu0KUkw0q5gKgMzN2iqxTp+2L09Zb2daV/Kx+H3zCgl2hBBCvDgUBc6vhX8/hNAb6nMVukDH78DGweiy+lO2AvA8OpZutfxTLF6fQhdSq292ZLrKSf3wRg0GmmghskmyOKCznTX9GhQ3y/3yCxmgLIQQIv/T6+H4YpjgFr8CctJAx6cq13psp/Xtt/j3fGiKRQDsvfxIdXwr5DlXHz4ztOjEpRDtnDPTzuQ2Kax4bG0lH+epkZYdIYQQ+Vf4fbi2G5b2MT5XZxDUfxecvRn2v12cvRPG4AVHuPpl2xSL0ybp2nryLJqXvowf0Dy8aUkalfFMMdgxl5Qmg6UUBIl4EuwIIYTIv35pDk+uGqf32wxFEnf8DktlFeOkNEmijUsPwg1fz9pyiVlbLD/FPOn9G5QqyM6LD4H07XH1IpNXRwghRP6iKHDyL5jsrw50ijWAj+/B+FBVoAPqICKpTWfucfXhM8NxTi9To9XAlK6VaFCqIF93q4qtlRZvF7sXdv2c9JKWHSGEEPnH6RWwtLc6rUAxeHs/WNuleJmpUGH3pYf0++2QKk2r0XDubhg/7rhCSU+nLFc3JR+0LMO09eeN0rUaDd1q+RsGSR//tIWqa02YJsGOEEKIvC/8AcxrCw+TBQi1+kOryaDL+A7fh64aTxlffOgGiw/dMJHbvLpUL0x1/wI42Oi4FxZpWKOnSAF7VT476xdvn6vMkGBHCCFE3qWPg8Pz4J+R6vQmn0CD9zK8KznEz6jSaTXE5eBSxNY6LYElPAzHy96ux+PwaIp6OOZYnfIyCXaEEELkTTumwZaJ6jRXf+i3CZy9Ml1slQkbeKdZKfQWnlkFUNDJhofh0UbpybumqvsXsHhd8jMZoCyEECLvOfCTcaBTqgWMOJG5QCdJbBEeFcuktWct1rJjZ5340fvDGzVU5+qV8OClkh4UcMh4t5tImbTsCCGEyBvunYY/e0DINXV6n7VQ7CWz385SLTuRMXrD1yU9nVXnFvSrA6Q8O0xkjrTsCCGEyP2ePYLZ9dSBjm91GHMzU4HOwauPqfvFZtadugOYno115k5YJisbr2Fp9Sai7waVpoKvC83LJ7Y8WevUd9ZoNBLoWIAEO0IIIXK320dhWrK9nhq8D33WgK2z6WvS0OfXA9wNi2TQH0d4a95BLj94ZpQnYcG+zHK2VXeevBNUin+GN1Ct1ZN0bE6TMhnbYV2knwQ7Qgghcq+Nn8KPjROPS7eGUcHQbCzYZH5mUkRMnOHrLefuZ6GCKatR1PSg4joB8bOsbHRaVbAzrFkpi9RDyJgdIYQQudUfXeHSpsTjOoOg9RSzFJ0ds8pfrlmEz9acMUp/I7AoLvbW1C3uLisfZxMJdoQQQuQuh+bCmhHqtLGPQJe3PrKSd2MlsNZpeblGEQDDbunCsvLWd44QQoj863kITClqnP7e+TwX6Hg626ZroLEMRs4eMmZHCCFEzntw3jjQqfQKvHcBnL1zpk5Z0KO2f4avsZIuLYvJW6GyEEKI/EVRYNuXsP1Ldfp75/NkkJOgfqmC6c7bvZYft0MjqejrasEavdgk2BFCCJEzFAW2Torf9iFByebw+l9mv9Wj8Cg+WXGKcj4u1ExhllRautX0M7kJ6P9eq4ZegUNXH/N245LcfBJBzWLu6S73y66VM1UfkX452o21Y8cO2rdvj6+vLxqNhhUrVqjOh4eHM3ToUIoUKYK9vT3ly5dnzpw5qjyRkZEMGTIEDw8PnJyc6Nq1K/fu3cvGpxBCCJFhigLbp6oDnbc2QM+lZin+yPUnPHkW/d+tFGpM3MS/p+4yY+MFJqw2niGVHlX93YzSNo1sRLvKvnSo4stnHSvi7WqnCnSq+MVfU6uY7G2Vk3I02Hn27BlVqlThu+++M3l+5MiRrFu3jj/++IOzZ88yYsQIhg4dyqpVqwx53n33XVavXs3SpUvZvn07t2/fpkuXLtn1CEIIITLjqzKw7Yv4r196B8Y9Af86kMkBu4evPeHCvacA7LjwgC7f76Hx9G0AJN/14fx/+bKqZtEClPR0SjXPT71q8GGrssx+vUaq+YRl5Wg3VuvWrWndunWK5/fs2UPv3r1p3LgxAAMGDOCHH37gwIEDdOjQgdDQUH755RcWLlxI06ZNAZg7dy7lypVj37591K1bNzseQwghREb8OxrC/2uBL9MGgiZkOsgBCI2IoevsPQCcmtCSzWfjyw59HgNArF6f4rUZtbB/HV77aT8AgSU80szv6WzH4MYlzHZ/kTm5ejZWvXr1WLVqFbdu3UJRFLZu3cqFCxdo0aIFAIcPHyYmJoagoCDDNWXLlsXf35+9e/fmVLWFEEKkZOVQ2D878bjjd5kKdC7df0q/+Qc5cTOEkOfRhvTNZ+8RHZcY3Pxz4g6vzDHf50G9EukfeCxyj1w9QPnbb79lwIABFClSBCsrK7RaLT/99BMNGzYE4O7du9jY2ODm5qa6zsvLi7t376ZYblRUFFFRUYbjsLCsbfYmhBAiDXExsHIInFicmDZoNziYHsgbGRPHkkM3aFLGEz93B9W5Lefu8da8QwBsOnufLe81Mpx7Z9ExVd4hC4+Yp/7ASxLo5Fm5PtjZt28fq1atomjRouzYsYMhQ4bg6+uras3JqMmTJzNhwgQz1lQIIUSKoiNgRlmIDE1MS2Nq+debLvDD9ivYW5/j7OetDOmPwqMMgU6CuOSDcsygrLcz5+4mju3Z/kFj/D0cUrlC5Ga5Nth5/vw5H330EcuXL6dt27YAVK5cmWPHjjF9+nSCgoLw9vYmOjqakJAQVevOvXv38PZO+YdozJgxjBw50nAcFhaGn5+fxZ5FCCFeWBfWw8JXE48bjoJGH6a5IvLuS/E7jj+PiSM2To+VLn7UxbOoOKO8sRYIdhYPDMTV3hq9XkEB2cMqj8u1Y3ZiYmKIiYlBq1VXUafTof9vsFmNGjWwtrZm8+bNhvPnz5/n+vXrBAYGpli2ra0tLi4uqn9CCCHMSFFg/cfqQKdwDWjyUYa3fhi55Ljh67+P3DQ6b4mWnYTVjLVajQQ6+UCOtuyEh4dz6dIlw3FwcDDHjh3D3d0df39/GjVqxAcffIC9vT1FixZl+/bt/Pbbb8yYMQMAV1dX+vbty8iRI3F3d8fFxYVhw4YRGBgoM7GEECKnxDyHX5rD3ZOJaT0WQ5lWKV+TilXHbzPtlcr8uusqMzdfNDpviZad1AKcHrX9WXP8Nm/UNbGPl8iVcjTYOXToEE2aNDEcJ3Qt9e7dm3nz5rFo0SLGjBlDz549efz4MUWLFmXSpEkMGjTIcM3XX3+NVqula9euREVF0bJlS77//vtsfxYhhBDEBzi/tICYiMS0nn9BqeZZKvbnncFMW3/e5Lk4M04tT5BasDO5SyU+71jB0LUmcj+NIvvLExYWhqurK6GhodKlJYQQmbV9GmydmHhs4wSjroCVrVHWaevPcTskkhmvVjG583e7b3dy6lbiTNmgcl5sOmt6dfzCbvbcCnmeqSp/1rEC41aeNkoPntxGdiTPA9L7+Z1rBygLIYTII04shWX91Gm2rjD6Wopr6Hy39TIAfesHULFw4gaYl+4/ZfifxzhzR70kiD6Vv8szG+gANCnjScsKD1l/Oj6QOj6uBVotEujkMxLsCCGEyBy9Hpb2hrOr1OkDtoFP1XQtFhgVq55dNXLJcaNAB2DLuftZqGjKbKy0VPcvYAh2XB2sLXIfkbMk2BFCCJE56z8yDnQ6zQHfaqlepk8yoFhR4jfqVJT4mU9h/23xkF2sdVrefCkArUZDw9KFsvXeIvtIsCOEECJjFAUO/qze9iFoPNTsC3Zpj3uMS9IlpQBdZ+8hToHlg+uZv67JvPVSAL/uDjYcW+s02Fhp6d+wuMXvLXKOBDtCCCHS7/ZR+LGxOu2DK+CY9qaYCZKui/PkWTRHrocA8OhZdApXmM9HbcomC3ZkRtWLQIIdIYQQaVMUWDMCDs9Tpw/em6FAB+DivXDD10kDnyWHbmShgumTfEq5jQQ7LwQJdoQQQqRt53TjQOft/eBZNtXLwiJj0Go0ONkmfty8u+SY4euNZxKnk09bfx4XO8t+LCWfZaWV1ZFfCBLsCCGEME2vh+9qw6NkqxYP3gte5dO8PCo2jsrjNwBw+Ys2RMfqsbfR8TA8ypBn2dFbqmvCImOzXO3egUXZfO4+N59kfkq6yF8k2BFCCGHswfn4QCcpN38YesjkIoFJfbH2LI+fRTMiqJQh7ZddV/hi7Tm+7VGNkAjLzrj6sHVZdl9+pEpb2K8OxQs5WfS+IveSYEcIIYSaqUCnw7dQpQfoUl+HRlEUftxxBYBOVQsb0r9Yew6AYX8eNW9dk/Bzt2fN0AY42FihS9JdtWd0U3zd7C12X5H7SbAjhBAiXmw0nF4Gywcmprn6wTvHQatLVxExcYkDjmMtsGdVamx0WsOigEmH5qQU6LzTrJTJdJH/SLAjhBACoiPgCx91WoXO8PLcdK2EnCBpgDPLxA7llpR09/PUNvJMYGstM7FeFPJOCyHEi+7cWuNA57Wl8Mq8NAOdU7dCmbHxAs+j47d9iIlNDDgS1s/JLrFxGQt2ZBvsF4e07AghxIsqOgLWvAsnFiWmuRWN77ZKI8gZueQYD55GsfPiQwBi4/QMbFSCoX8esWSNDX7uVZMfd1zhwNXHhrSka/ZoU6l/eR8XztwJo00lnxTziPxFWnaEEOJFdOCn+NacpIFO7QEwZH+6uq2WHbllCHQATt8O4+uNF1Rp5qDVwGt1/LFL1uUUVN7LKG/SLrQqRVyNzidYOfQlDn8SREBBR/NVVORq0rIjhBAvmstbYe376rQB29LcwDPBExPbOijA7RDzr2tz+JPmFHC04YvOlSg2+h/VuQ5VfVUtO0nH7HzQqizOdtYmW2+sdVo8nFKfPi/yFwl2hBDiRaHXw/YvYfuUxLTXl0HJZhkqpvuP+4zSdlx4kNXamaTTJbYyFXK25cHTxAUJX6vtT0BBR3r+vB+AuCRjdpxsrXi/ZRmL1EnkPRLsCCHEi+DuSZhTX502YDv4Vk13EZExcWw8c4/z956at26pSLpejs7EVg8vlSxoOI7J5qnuIu+QYEcIIfK75DuVOxaC/lvBzS9DxUxee5b5e6+Zt25pSDqryt/dgbthkSnmTTpAWYikZICyEELkZ1e2qQOdEs3gvQsZDnQgflCypR0b15zutRLrljTY+erVKjQt68nC/nVMXpt0QUMhkpKWHSGEyK/Wfwx7/5d4XKoF9Fya6eIsHUpoNeDmYKMKcJJ2Xfm5O/Brn1oWroXIj6RlRwgh8qOjC9SBTtAE6LEo5fxpiInTEx1r2TExNYoWAECfZLU/bToWB5zdszqu9tb89lbtNPOKF5O07AghRH4S8RgWvw7XdiemffIArGwyXWScXqHh1K1Ex1ku2OlbP4ABDYsD8Hrdovx54AZB5TzTdW3rSj60quiNJgPbWogXiwQ7QgiRXxxfDMsHqNNGX89SoAPwJCKaO6EpDwzOqv4NAvi4bXnDcQVfV/aOaUrBDKyFI4GOSI10YwkhRH5wfb9xoDPmJtilvJJwcrFxemJNtN6ktvVCelX1c+ODFNa9SRroJPBxtcdaJx9RwjzkO0kIIfKy6Gfwv1rwa4vEtEqvwjsnwNYZgIv3nvLWvIOcuBmSYjHLj96k5Mf/Um7cOlYeu8XA3w8RHhULmGdKd42iBbDWSeuLyBnSjSWEEHlVzHP4swc8vJCY1v1PKNtGla3Xrwe4ExrJ9gsPuPxFG0x5d/Hx+CLjFN5ZdAyA9Z+up1dgURxts/5RYWulpVUFH75Ye44yXs7ZujChEBLsCCFEXnR8ESwfqE57818oWs8oa8J4m8y00PyWyUUEBzUqwbbz9zl3Nz6oqeZfAH8PBw5/EoSznTU/77rC1HXnebtxiUyVL0RGSLAjhBB5SfgDWNAV7hxPTCvWIH79HGt7Q9LtkOfcfxpFVT83k8XE6RVCIqItsiFm8UKOjG5dltO3Qw3BTsLMqoT7DW5UgvaVfSlSwD7FcoQwFwl2hBAirwi5Dt9UUqd1mg1VekCyQcT1vtwCwMZ3G5osasBvh9h87j6rh9anUpH0D2JOj6iY+EHOtlY6Q1ry2VIajQY/dwez3leIlMgAZSGEyO0UBY78ZhzovH8Jqr5mFOgkdexGiMn0zefuA/D7vqtmqmSiZ9HxA5ttreUjRuQO0rIjhBC5laLA2g/g4E/q9OafQ+BQ0KYdTJgapXPqVqjha3NMK08uIjoOiB+ULERuIMGOEELkRlHhMLmwcXpAQ3hpePrLMRHttPt2V+JpBQ4EP85EBWH10Po42Opo9tV2Vbrff+NwknZjCZGTJNgRQojcJjIUfutonN5qCnG1B3Ll3lNKejqla9VgJVm0EzRDHZgsPnSDxYduZLiKn7YvbzTWp2UFLzRoeP+/xQNfq+3PnweuU7uYe4bLF8KcJNgRQojc5MhvsGqYOq3dN1DzTQA+WXaSPw9cZ1SrMrzduGSaxSnJWnYu3Q83SzWbl/cyfP1p+/JsOH2PGa9WVa3JU6mIKwc+aoa7Y9a2qxAiq6RDVQghcgNFgX9HqwOdKj3gg8uGQAfgzwPXAfhm40XV5Q/Do1CSRzYWlHSsz5svBfDngLomFx/0dLHDSrZ9EDlMWnaEECKnrR0FB35Qp738K1TsqkpS7VuVpAdr05l79PvtEN1r+TG2XXkm/nPGcG70spOWqDE6rWz9IPIOCXaEECKn6PXwpT9EJ9k6QWcDg/dAwVJG2advSNwWImmoMWNjfPqigzfwdrXjzwMZH4OTHoMalWDO9suAZWZxCWEp0rYohBA54e5J+KyAOtBpMQnGPjAKdEIjYjgQ/NgQaABExep5/t8U76QbbO659MhiVe5ey8/wtcQ6Ii+Rlh0hhMhul7fA750Tj70rQ9+NYG1nMnvrmTu4/d/+Vkk1mLqVQ58Ecfxm4ro5BZ2zNhj4tTr+xMTqWXr4ptE5P3cHvF3s0Gk1uNlbZ+k+QmQnadkRQojsFPFYHejUHQKDdqYY6AAmAx2IH5ScnLNt1oKQSZ0qUrNYAZPndFoNO0Y1YfN7jWTQschT5LtVCCGyy8m/YGpA4nHgUGj1hVlvYW+TtYX8NBqNyfV7fnyjBgA2VlrsrGWxQJG3SDeWEEJYmqLA1i9gx9TEtALFoMXEVC+7E/qcmNjUp5P/vu+a6njenquZrGQiU8NxKhdxy3K5QuQUCXaEEMKS7p+F7+uq01yKwJCDqY7y1esVAidvSbP4sStOZbWGRqxNdFFZ6WREssi7JNgRQghLubYX5rZSp7WfBTV6p3lprD77FghM4OlsC6QQ7Mi6OiIPkzE7QghhCccXqQOdBu/D+FCo0ZsF+6/x+Zozqa54rLfAasjTX6li+NpU7NKhii8QPy4nORmQLPIyadkRQghzu7gRlg9MPO78I1TpBsSvgvzx8viup+blvahb3MPo8pM3Q/FwMv9+Ur6udgRPbsPJW6F4OttRd/Jm1fmEgMbaRJeVtOyIvEyCHSGEMJeHF+F/NROP/QOh2wJwjA9onkXFUnnCBsPpJ8+ijYo4ePUxr8zZa5btGGoWLcCha08Mxzpt/EyrhMHGm0Y2xNZKR4OpW4HEIMfGRCuObA8h8jIJdoQQwhwiHqsDHY+S8MYK1fo5XWfvIS7JWJzopHtd/Wfz2fsAqnyZ5eagbh1KHrCU9HRWHSds5GmdpBsroKAjTrZW0rIj8rQsBTuRkZHY2aW8EJYQQrwQTM246rfZaKHAc3efqo5j4xSeR8cxZd05WlbwJrCEcZdWVrgmW+U4pdaZ91uUZuPZ+7xetygApb0Sg6BNIxuhAZNr7wiRV2R4xJler+fzzz+ncOHCODk5ceXKFQDGjh3LL7/8YvYKCiFErhXxGMa7qgOdvpviByLbu6V5eXScnl92XWHenqv0+GkfAArmG5icfKCxldb0r/yhTUuxcshLOP3XsuNqb83+j5pxbFxzdFoNWmnVEXlchoOdiRMnMm/ePKZOnYqNTWITacWKFfn555/NWjkhhMiV9HpY/7F6NWSAhqPAr1a6i4mJ03MrJHEriHthpreFyKyAgg6q44w0zni52Bl1gwmRV2U42Pntt9/48ccf6dmzJzpd4pLhVapU4dy5cxkqa8eOHbRv3x5fX180Gg0rVqwwynP27Fk6dOiAq6srjo6O1KpVi+vXrxvOR0ZGMmTIEDw8PHBycqJr167cu3cvo48lhBDpoygwOxD2/k+d3n8rNP04xctO3Qo1SouJU3CyTfw92vm73Sw5eMMs1exTrxi96xVTpVliOrsQeUGGg51bt25RsmRJo3S9Xk9MTEyGynr27BlVqlThu+++M3n+8uXL1K9fn7Jly7Jt2zZOnDjB2LFjVeOE3n33XVavXs3SpUvZvn07t2/fpkuXLhl7KCGESI+QGzDBDR4k+cPu1d9gfChTTjrw/tLjJtfOWXfqDu2+3WWUHhOnx9YqMdi5HRrJk4iM/R5NyfgOFVRlg3kGPQuRF2V4gHL58uXZuXMnRYsWVaX/9ddfVKtWLUNltW7dmtatW6d4/uOPP6ZNmzZMnZq4n0yJEiUMX4eGhvLLL7+wcOFCmjZtCsDcuXMpV64c+/bto27dukZlCiFEpoTdgW8qqtM+DTH0Dc3edhmAAQ2Lqwb4Agz644jJIg9fe0JJTyezV1V170YlmLM9vm7SsiNeVBlu2Rk3bhxDhw5lypQp6PV6li1bRv/+/Zk0aRLjxo0zW8X0ej3//PMPpUuXpmXLlnh6elKnTh1VV9fhw4eJiYkhKCjIkFa2bFn8/f3Zu3dvimVHRUURFham+ieEECkKvQUzyiYeF2sAH98zBDpJW0xi4vTE6RU2n73Ho/Aojt0ISbHYjWfuce3RM7NV8+tuVQgq58XcPonjhka3Tqy3s521qcuEyPcyHOx07NiR1atXs2nTJhwdHRk3bhxnz55l9erVNG/e3GwVu3//PuHh4Xz55Ze0atWKDRs20LlzZ7p06cL27dsBuHv3LjY2Nri5uamu9fLy4u7duymWPXnyZFxdXQ3//Pz8zFZvIUQ+ExUOX5dPPK74MvRZo5pWHqtPXC/n5pPnzN52ib7zD1Fj4iY6fbc71eJPmhjLkx7vNCtllNa5WhF+7l2TJmU9VekzXq3CqFZljFqchHhRZGqdnQYNGrBx40Zz10VF/98vj44dO/Luu+8CULVqVfbs2cOcOXNo1KhRpsseM2YMI0eONByHhYVJwCOEMPY8BKYk6bKvPQDaTDMcztp8Eb2icP1xhCFt4O+HM3SLB0+jMlW11+r4U6SAPR/8dSLNvF2qF8nUPYTILzIc7Bw8eBC9Xk+dOnVU6fv370en01GzZs0UrsyYggULYmVlRfny5VXp5cqVY9eu+IF+3t7eREdHExISomrduXfvHt7e3imWbWtri62trVnqKYTIh54/gX9Hw4lFiWl+dVWBzrEbIczYeCHLt4qMMV5FOT1sdFpeqemXrmBHiBddhruxhgwZwo0bxlMjb926xZAhQ8xSKQAbGxtq1arF+fPnVekXLlwwDI6uUaMG1tbWbN6cuJnd+fPnuX79OoGBgWarixDiBXJmFUwppg50Wn0Jfdersj2JMN7XKjvp/tvHqm1lnxythxB5QYZbds6cOUP16tWN0qtVq8aZM2cyVFZ4eDiXLl0yHAcHB3Ps2DHc3d3x9/fngw8+oFu3bjRs2JAmTZqwbt06Vq9ezbZt2wBwdXWlb9++jBw5End3d1xcXBg2bBiBgYEyE0sIkXFnV8OSN9RpXX+BSi8TEhHNDzuu0KVaYUp5OaOz8PYJdYu7s+/KY1Vas7Ke+LjZoUGDy3+DjRuXLsQ/J+5YtC5C5HUZDnZsbW25d+8exYsXV6XfuXMHK6uMFXfo0CGaNGliOE4YR9O7d2/mzZtH586dmTNnDpMnT2b48OGUKVOGv//+m/r16xuu+frrr9FqtXTt2pWoqChatmzJ999/n9HHEkK8yBQFVr8DR+YnpnX/E8q2MRx+uuo0K4/dZva2y1z9sq3FdwGv4OtqCHZalPeiY9XCNCpTyLClQ4Ku1YtgY6Wlml8Bi9ZHiLxMo5haASsVPXr04M6dO6xcuRJXV1cAQkJC6NSpE56enixZssQiFbWksLAwXF1dCQ0NxcXFJaerI4TITooCn7mDkmTszPBj4K7eCqLRtK1cexQ/EPnql23Zc/khr/2032LVGtqkJP/bGt/y/XOvmgSV97LYvYTIq9L7+Z3hlp3p06fTsGFDihYtalhE8NixY3h5efH7779nvsZCCJHdIkPhx8bJAp2jRoEOgDZZt1XyY3Mr4elo+FoCHSGyJsPBTuHChTlx4gQLFizg+PHj2Nvb8+abb9KjRw+srWXBKiFEHnH3FPzRBcL/20vPrw68thjsjbuDlGTTyyHzU8bTq2OVwly+/4waxaR7SoisytQ6O46OjgwYMMDcdRFCiOxx4wD8kmQR1DqDoPWUFLN/vfGC0b5Sw/48aqnaAaDVani/ZRmL3kOIF0W6gp1Vq1bRunVrrK2tWbVqVap5O3ToYJaKCSGERZz6G/56K/G45lvQcnKql8zackl1/OSZeaadF3CwNtvGn0KIlKUr2OnUqRN3797F09OTTp06pZhPo9EQFxdnrroJIYR5zakPd08mHpsYiJwe1T43zwrymmTjftpX8WXHhQcmt4IQQmReuoIdfZJ9X5J+LYQQeYI+DpYPVAc67WelGeicvRPG8xjL/QGXfIhz+8o+zOxWFa2Fp7UL8aLJ0JidmJgYWrVqxZw5cyhVSv7yEELkAbcOw09NE491tqxsd4SHz2Lo+1/SnksPcXOw4fGzaCoWdsHNwYbFB6/z4d8nTRZpLkkbdj5oWYbm5b2MWnuEEFmXoWDH2tqaEydkHxYhRB4Qch22T4WjSZbEaDkZ6g7mnTFrgfhVip1trXntZ/V6OTY6LdFxlm/Fbl3Rh9/3XaOkpxNDmpS0+P2EeFFleDbW66+/zi+//MKXX35pifoIIUTWREfAwlfh6k5VclT77/g7tiHNkkwZP3kzlNHLjFtvMhvoBBR0JPjhs3Tn/6hNOSoWdqFpWVlHRwhLynCwExsby6+//sqmTZuoUaMGjo6OqvMzZswwW+WEECLdru+D7VPg8hbjc2/vY8LuWBbuP0mRbfaGZHPsWp7g844VmLfnarrzt6vsg72Njm61/M1WByGEaRkOdk6dOmXYCPTCBfUvCulrFkJkO0WBFYPh+J/G51p+AdV7ga0zG89sAuDmk+eG008jY81WDW9Xe9Wqym0qebP25N0U80/qVMls9xZCpC7Dwc7WrVstUQ8hhMi4qHCYXFidZu0AnWZDhU6qZFPbACZfKDArrLQaVbDTrZY/RQo4EFjcg5shzxm74pQ6v07+OBQiu2Qo2Fm8eDGrVq0iOjqaZs2aMWjQIEvVSwghUhd6E76uoE4bdgQ8ShgOHzyNQqOBgk62JgObWDMupeFkZ0X32n5MWH0GgMDiHjQqXQiIHxuUnKX31hJCJEp3sDN79myGDBlCqVKlsLe3Z9myZVy+fJlp06ZZsn5CCGHs8RWY2zbx2MkrfoFAGwdDUmRMHLUmxXddXZzU2uRKxZlp2HnrpQBCIqJZdvSWKt3J1oregcWoWNiVcj4u2FhpDedMxTUS6wiRfbRpZ4n3v//9j08//ZTz589z7Ngx5s+fz/fff2/JugkhhFr4fVjaB2ZVg6e349OajYP3L6gCHYCQJMHNqVvGLSuZNahxcRxsdUbp/u4OaLUaahVzx8lW/XdkUQ8Ho/wmetWEEBaS7padK1eu0Lt3b8Pxa6+9Rt++fblz5w4+Pj4WqZwQQhgc+R1WDVWnvb0fPMuazK5N8qdc5+/3mKUKv/apiaeznVEX1MohL+Fom/KvU2c7a3aPboq1TkP/3w5jZ6XFzjrdf2sKIbIo3cFOVFSUapq5VqvFxsaG58+fp3KVEEJkUcJWDyeXJqZZ2cHbe8G9uFH2kIhohiw8Qr0SBc1eFTvr+Bad5D1QVfzc0ry2sFv8lPcVb9eLL0P6sYTINhkaoDx27FgcHBKbY6Ojo5k0aRKurq6GNFlnRwhhNs8ewoKX4fbRxLTuC6Fs4nid07dD+WnHFd5rUQY/dwd+2HGF3ZcesfvSI7NXx9bqv2AnC4GKBDlCZL90BzsNGzbk/PnzqrR69epx5coVw7H8EAshzEJR4IcG6o07bV1h4HajzTs7/m83sXqFC/fCWftOAyItuHGn7X+DjuVXnRB5S7qDnW3btlmwGkII8Z+wO/Bri/i9rRK0mAT1hprMHvvflKrz954C4GiT4eXD0i0h2JFp40LkLZb7rSCEEBl16wj81ESd1ncj+NUG4E7oc07fCqNZOU80Gg2xSfawSlhHx9RMKXPRauODHAl1hMhbJNgRQuQOxxfD8gGJx80/h5eGq7IETo7f92pq18qU93Xhj33XVOdDn8ew7fyDLFelYmEXTt0KS/G8p4ttlu8hhMg+EuwIIXJW6C34tRWEJum26rcZitQE4OydMNafvsvAhokrI4/6+4TJoup/uYWnUVnf70qTQtuNt4sdAL0Ci/HF2nNZvo8QIntIsCOEyDnPQ+DP7omBjr07jDgBts6GLK1n7gQgMibtrR3MEegk97/XqlG7mDsxesWwlo6dtY66xd3Zd+Wx2e8nhDC/DK9qFRNjvOR6gocPH2apMkKIF0joTfihIdz9r5Wmei/+bbyalt8f5cqDcKPsp2+bbxXkjLC31uHpYmdYJ0cIkfdkONjp3r27yd2D7927R+PGjc1RJyFEfrdlUvwmniHXQGsNb22ADt8yePk1zt97yuhl8VPOd17M+vibzEg62apYQUfTeWSYshB5RoaDnevXr9OvXz9V2t27d2ncuDFly5petl0IIRLod82EHVMTE16ZB/51VHmeR8evlTN1nXptL3P5qVfNdK+VU6KQk8n0IgWkpUeIvCLDwc7atWvZs2cPI0eOBOD27ds0atSISpUqsWTJErNXUAiRT5xZBeNd0W4aB4Be0cCIU1CunVHWk7dCefIsmjuhkYa0Kw+eZbkKDUoV5Nse1Whe3ovpL1dRnWtd0ZtiHg782b9uusr6qE05OlTx5Y++ddLOLITIURkeoFyoUCE2bNhA/fr1AVizZg3Vq1dnwYIFaLWysZ0QIpmw27BmJFz415C0Ja4qI2MGc8zNL8XLBvx+iIfhUYbjWyFZ24dvZveqdKxa2HBsm2QjzqBynkx/pUqqm3kmV8DRhlk9qmWpTkKI7JGp2Vh+fn5s3LiRBg0a0Lx5c37//XfZKkIIYez8OvizmyppT/F36HemFnq0PI+Ow97G9CKAB68+MVs1Lk1qjZVO/ceYLsnvrJ9711Kdk99mQuQv6Qp2ChQoYDKYiYiIYPXq1Xh4eBjSHj+WqZhCCGDrZNj+ZeJxqRbQYhIHj2vQn7kAQI2JGznzWSsADl8zX3CTXPJAB6BJWU8CCjpSqbCriSuEEPlJuoKdb775xsLVEELkKztnqAOdbgs47FCPv3be4nKSaeUR0XHM2X6ZkIgY5my/bJGq+LjamUy3s9ax5b1G0iotxAsgXcFO7969LV0PIUR+cG0PzG1tOIx0K4Xd29vBxpGuo/8xecmX/2Z+JeLO1Qqz/OitVPP89lbtFM+lFOgEFHTk+M2cWddHCGF+GR6zs3btWnQ6HS1btlSlb9iwgbi4OFq3bp3ClUKIfEsfB3/3hdPLDUkLYpvxa+wwNts4cvS6Zbqo+jUISDPYKelpeup4asa1r4C1TsurtVIeQC2EyDsyPH1q9OjRxMXFGaXr9XpGjx5tlkoJIfKQqKfwmbsq0Bkd04+PY/ui1WoJfR5D5+/3WOTWOq2Gcj4uqebJTDeVu6MN016pQq1i7pmtmhAiF8lwsHPx4kXKly9vlF62bFkuXbpklkoJIfKIsNswuUjiccEyMOIki+KaAvErEV+6b7z1g7lo0PBBy9JG6V+9UsVEbiHEiyrDwY6rqytXrlwxSr906RKOjqaXVRdC5ENn18CMconHFTrD0APg5m9I0qDhUZK1csxNQcHE7jXIkl9CiKQy/CuhY8eOjBgxgsuXE2dOXLp0iffee48OHTqYtXJCiFwo/D586Q+Le8YfKnb85vUhvDzXKOv5e09ZfPCGxaoSpzcR6YDJAEgI8eLKcLAzdepUHB0dKVu2LAEBAQQEBFCuXDk8PDyYPn26JeoohMgtgnfC9FIQmThTqWP054y7VoWUNpvafO6+xaqjKFC5iJtRuncK082FEC+mDM/GcnV1Zc+ePWzcuJHjx49jb29P5cqVadiwoSXqJ4TIDfR6WPSaassH6gxieEh3Lh+/bUj6busl/N0dsq1aNlZaCjnbsndMUwInbzGkBxb3YFSrMpTxcs62ugghcq9MbReh0Who0aIFLVq0MHd9hBC5SWw07J4JWyeq0zt8C9V7Ef37YUPSwauPmbbeMruUm9IrsCil/ptW7uOq3oFco9HwduOS2VYXIUTulqlhfNu3b6d9+/aULFmSkiVL0qFDB3bu3GnuugkhclJMJEwspA503Pxh3GOo3guA6Di94dTbC46Y7daHPglKM89nHSuqppW3q+xjtvsLIfKXDAc7f/zxB0FBQTg4ODB8+HCGDx+Ovb09zZo1Y+HChZaooxAiux2aC5O81GldfoIhB0GbuHFndGxisPPgqflmXRV0sk31vIudcaO0XkYlCyFSkOFurEmTJjF16lTeffddQ9rw4cOZMWMGn3/+Oa+99ppZKyiEyEbRz+ALX3VaySDo+ZdqALJer3DlYbiqZSezvFxsuReWvkDpyhdt+OfkHaoXLWB07o26xVh78i4NSxfKcp2EEPlLhlt2rly5Qvv27Y3SO3ToQHBwsFkqJYTIfveunVMHOhodDNwBr/9tNNNqyvpzBM3YwYHgx1m+b/da/ibT141oYJSm1WpoX8WXwm72RucCS3iwd0xTfu1dM8t1EkLkLxkOdvz8/Ni8ebNR+qZNm/Dzk31khMhr7odF8uNP3+E1t476xMd3wMd4JeLImDh+2G68sGhmNC/vxdCmpgcSuzvYZLg8H1d7rHSyoqAQQi3D3Vjvvfcew4cP59ixY9SrVw+A3bt3M2/ePGbOnGn2CgohLGj/j3j++wEDkiQFu9cnYNASsDI9bmbKuszvUp7c+y3KYK3TsnNUE9p9u4vQ5zGGc54udrzXvDRfbbxgtvsJIV5MGQ52Bg8ejLe3N1999RVLliwBoFy5cixevJiOHTuavYJCCAt4HAyr34Hg7ark96IHUbRCP4bbpLz1y9+Hb5qlCv8Mr08Z7/h1cPzcHXi5RhF+2aXuCh/WrBQPwqP4be81s9xTCPFiytQ6O507d6Zz587mrosQIjs8vgKzqqmSjupL0jV6PHq0sPEC3q52vFpT3S0dER3L8+i4FLdoyIifetWkgq+rKu3d5qWJ0yu0r6KeQl68oOy5J4TImgwHO8WLF+fgwYN4eHio0kNCQqhevbrJTUKFELlAXEz8KsgXNySmeVeiXeRETt19pso66q8TqmDnQPBjXv1hr9mq4mxi6riTrRXjO1QwSu9ZtyhXH0XQsHRBs91fCPFiyXCwc/XqVeLi4ozSo6KiuHXrllkqJYQwswM/wdr31WntZ0Ll7iizD6Z42ZHrT5i/5yqHrj4xa3WcbNP/q8dapzUZBAkhRHql+zfOqlWrDF+vX78eV9fEJui4uDg2b95MsWLFMnTzHTt2MG3aNA4fPsydO3dYvnw5nTp1Mpl30KBB/PDDD3z99deMGDHCkP748WOGDRvG6tWr0Wq1dO3alZkzZ+Lk5JShugiRLwXvhPnt1GnWjvFTygvGz4LSprCBJ0C3H/YSE2f+xfocMxDsCCFEVqX7N05CEKLRaOjdu7fqnLW1NcWKFeOrr77K0M2fPXtGlSpVeOutt+jSpUuK+ZYvX86+ffvw9fU1OtezZ0/u3LnDxo0biYmJ4c0332TAgAGymrN4cT0PgaN/wKFf4sfnJNX1F6j0sipJm0Kso9crFgl0IH4DTyGEyC7pDnb0+viVUgMCAjh48CAFC2a9/7x169a0bt061Ty3bt1i2LBhrF+/nrZt26rOnT17lnXr1nHw4EFq1oxfSOzbb7+lTZs2TJ8+3WRwJES+du80zK6nTnPwgJfnQvFGhiRFUQz7SmlSaNn5eMXJLFXlpZIezO1Tm9KfJO6UXryQIzFxerycU98OQgghzCnDbcnZuUqyXq/njTfe4IMPPqBCBeM++7179+Lm5mYIdACCgoLQarXs379fZoyJF0dMpPFeVgDtvoYab6pWQH4UHkXrmTtpU8mH8R0qpNiy8+eBG1mqUjEPR6MWnPUjGqIoyMJ/Qohsle7fOHv37mXNmjWqtN9++42AgAA8PT0ZMGAAUVHm2wgQYMqUKVhZWTF8+HCT5+/evYunp6cqzcrKCnd3d+7evZtiuVFRUYSFhan+CZEnPX8Cs+sbBzrd/oDxoVDzLaOtHhbsv879p1HM23MVgGM3QixStYTblvNxAaCAgzXWOq10YQkhsl26f+t89tlnnD592nB88uRJ+vbtS1BQEKNHj2b16tVMnjzZbBU7fPgwM2fOZN68eSk2s2fW5MmTcXV1NfyTbS5EnqPXw5VtMKUY3EvW3fThNShnvH+d4dJku4ObYdkcvF3sjNISpq7/1KsGPWr7s3RQPaM8QgiRHdId7Bw7doxmzZoZjhctWkSdOnX46aefGDlyJLNmzTKsqGwOO3fu5P79+/j7+2NlZYWVlRXXrl3jvffeM8z68vb25v79+6rrYmNjefz4Md7e3imWPWbMGEJDQw3/btzIWnO9ENkqMhQ+KwC/JVuxfNDu+NYce7dUL086+0pRsh7pjGtXnojoWMPxvjHNOPBRMyoXia9HkQIOTO5SiZKeMkNSCJEz0j1m58mTJ3h5JTaVb9++XTW4uFatWmYNGt544w2CgoJUaS1btuSNN97gzTffBCAwMJCQkBAOHz5MjRo1ANiyZQt6vZ46deoYlZnA1tYWW1sZICnymJAbsG82HJ6bmGbrAh1mQQX1+LSYOD1Npm8jOlbPzg+bYGulM5zbH/zI8HVUrD7L1dJq1OV4uxq38gghRE5Kd7Dj5eVFcHAwfn5+REdHc+TIESZMmGA4//TpU6ytrTN08/DwcC5dumQ4Dg4O5tixY7i7u+Pv72+0SrO1tTXe3t6UKVMGiN+Tq1WrVvTv3585c+YQExPD0KFD6d69u8zEEvnLlomwY5o6rfln8NI7JrMvPXSTm0+eAzB391UGNSoBwLVHz9h9KUmwE5P1YEej0dChii9LD9+kZtECWS5PCCHMLd3BTps2bRg9ejRTpkxhxYoVODg40KBBA8P5EydOUKJEiQzd/NChQzRp0sRwPHLkSAB69+7NvHnz0lXGggULGDp0KM2aNTMsKjhr1qwM1UOIXOvKNuPuqjbT42dY6VL+8X3wNHGywLVHiVtBJARACd5dcixD1elQxZcvulTi1Tl7OXMnfmC/VgOfdqhArWLuNC9vYkaYEELksHQHO59//jldunShUaNGODk5MX/+fGxsbAznf/31V1q0aJGhmzdu3DhDYwauXr1qlObu7i4LCIr8J+Ix/NoKHp5XJV9sMJNStfukeblC4s/Vnwdu0Ki0J60qehutXLzl3P3kl6ZKryg42Vqx9p0GFBv9T3yiRoOTrRWv1pKB/kKI3CndwU7BggXZsWMHoaGhODk5odPpVOeXLl0qWzQIkVWKAvvnwLrRquR7JV+l4ak2RG204WqzpNnjg5rkMxaT/w0x6I/DlPV2pl+D4lmuXnKFnGT8mxAid8vwooJJ98RKyt3dPcuVEeKFpo+DiZ6gT5zZRJ3BUGcgmy/qiDqlnmKuKAq9fj1AWGQsywbXQ/ff6oDPomLZduGBUfHn7j7l/aXHs1TFpDHVrB7VOH4jhBbSdSWEyOVkNz4hcoPQm7CklzrQeecEFCgKgEZz3eiSWL3CzosPAbjyIJzoOD3n7jxl6eEbHLfQQoFlvZ0NX3eo4kuHKjIRQAiR+0mwI0RO2/sdrP8o8bhAALxzTJXF1JYOcUlWAwyPiqXz93ssVMFEfetnrRtMCCFygqzbLkROiYuB+e3Vgc5rS2DYEaOsSbd0CIuMofP3u/lpR+KO5l+sPWu2as15vYbJ9LaVfLC30Zk8J4QQuZm07AiR3RQF1ryrXhzQwQMG7gTXwkbZH4VHqTbl/GVnMEevh3D0eogh7eDVJ2ap2sCGxWlV0fTq48528utCCJE3yW8vIbJLTCRc3ABL3lAlK3XfZrVHH4qHO1HRxPj/u2GRquNnUbHGmcxkaNOSquPPOlbA0caKRQev816LMha7rxBCWJIEO0Jkl4WvQPAOddrAHewOL8zwX/YDl7j6ZVvDKb1e4e0FR4iOU69yHGeG/axM2fhuQ5zt1KugVyrsSjX/AnStUcQi9xRCiOwgwY4QlvTsIWwcB8cWqNPbz4TqvUGj4fyuYEPyjccR+LrZo9NqOHL9CetO3zUqMjbOMsFOKa/EmVYrh7zE9ccRVPOX7R+EEHmfBDtCWIKiwOnl8Neb6vSab0G7r1VJSSdaNZi6lXaVfWhbyYcYvemgJiYu6/tZpaWKnxtV/Nwsfh8hhMgOEuwIYW4h12HF23B1Z2KaQ0FoNg6q9zLKnnxa+ZoTd1hz4k6KxSfv1jKHpOvnCCFEfiPBjhDmEvMcZlaB8Hvq9GafQoORRtkVReHPAzcMG2qm+zYZ7Mayt9bxPCbO5DkfVzv6NShO78CiGSpTCCHyEgl2hDCHu6dgzkuJxxod9FoJAQ2A+MBmwf7rlPNxpkbR+K1V1p26y0fLT5oqLVXRsaYDl5R8070qA38/bPJcdf8C9K0fkOE6CCFEXiLBjhBZoSiwbACcXKJOH3UZ7BMH9+68+JBPVpwCMMy4On07Yy06Cdafvpd2piRsrFJeO9RBFgkUQrwAJNgRIjOeh8CN/bBuDDy+nJhe/10IGm+U/dL9cKM0S00hT85GlxjseDja8OhZtOG4tJeM1RFC5H8S7AiREXeOw8Ju8DTZAOLCNaHb7+BiemNMU2HNwv3Gm3tagi7JCOjvelZn0j9nKePtjKu9Nb3rFcuWOgghRE6SYEeI9HgcDL+2gvBk697UfAvqDoGCJU1f9x/FRCtO6PMYc9YwlXsnfl3Vz43Vw+pny32FECK3kGBHiNTERMLumbDtC3W6W1FoOQnKtTe+JE5P3/mHqOrnxsjmpbOpoikr7GZP91p+uDnYYGctY3SEEC8eCXaEMEVRYNuXsP1LdXq59vDyPNCl/KOz+ex9dlx4wI4LDwiNiOb4zVCal/cynL947ynbzj+wSLVHty7L8Rsh/HsqsQVKo4Evu1a2yP2EECIvkGBHiKQUBYK3w28dE9O01lDtdWjwHrj5pVlE0kX/5u+9BoBVknEzzb/eYXSNOTjZWjGoUQkAtl94QO9fD1jkPkIIkddIsCNEgqhwmFxYnWbtAP23gGe5dBej02iM0g5de5LV2hm806wUMzdfVKW1rujN6NZlDcd1AtzNdj8hhMjrUl6AQ4gXxbNH8WvlJA902s+Cj+9kKNAB0Fn4p+rlZDuQl/NxYfbrNSjq4WhISzrd3NPF1rIVEkKIXE5adsSLS6+Hs6tgaW91eoXO0PVX0GYuatGaaNkxJ22yzbTeeqmYyTynJ7REryjYWsmgZCHEi02CHfHiURQ4PBfWvKtOdvXjWbs5OJXK+NTsqNg4bHRaNBqNxYOd5N1kyVt6Ejjayo+3EEKABDviRXPqb/jrLXWanRv0+JPmf8dw6ZdQDnwciaezXbqLvBcWSb0vtxBUzhM7a53F1s8p6+3M1JcrG+2SrrFwcCWEEHmdBDvixXBuLawbDSHX1Ond/4SybQC4dP8fALadf8CrNdOedZVgycEbxOmVDO9ZlVFTulamchE3HjyNsuh9hBAiv5FgR+RvUeHwa0u4d0qdPngveJU3eYmp1Y5TE6PPnj2uErZ9SN6yI4QQInUyG0vkT4oCe/4XP8MqaaBTbziMfZhioAOQZJkc7oZGsvLYLWKSJiYRGRPHrGTTwC0lIdhJ2m31Qcsy2XJvIYTIy6RlR+Q/kWGwaiicWZmYVqwBvLEi1ZWPEyTdjbzF19sJi4zlflgU/RsWN8r77RbzBjpXv2xLsdH/mDyXsDBh0panbrXS390mhBAvKgl2RP4RGQZ/94WLGxLTXP1h0E6wd0t3Mfok3VJhkbEA7Lj4wGSwc+xGSGZrm2HJp5wDSI+WEEKkTbqxRN4Xfh9+bAJf+iUGOjbO0HoqjDiRoUAHQK8oRuN2rE2sFKg381id8e2Nu9a+eqVKqtdkz2ghIYTI26RlR+RdigJnVsDSPup0z/LQ5x9wyNyWCYsP3mDC6jOqtKQrEiuKQmSMnlYzd3DtUUSm7pFcWW9n+rwUAMCc16sz8Z+zfNujGqW8nHlv6fH/7huf193RxrAdhIejjVnuL4QQ+ZkEOyJviomMX/n4wrrENHt36LUSfDK+w3fSlpxzd58anbex0vIsKpbBC46w48IDHGx0RETHZarqAAMbFqeIuwNjV8QPnn4WHWs416qiD60q+gDxA6CT1BKIH6C8aEBdw9dCCCFSJ8GOyFuePYqfSv4oycBgr4rQegoUy9jKx8EPn/E0MobKRdzSHHtjrdPy2eoz7LjwACDTgc7sntUp4elEaS9nAEOwE/Y81mR+XQrzzCXIEUKI9JNgR+QdNw7AL83Vab3XQECDTBXXZPo2APaNaUbn7/ekmtdap2HRwRuZuk9SrSv5mExPKahJujWE7HElhBCZI8GOyP2eh8CUouq0Cp2h1RRw9spy8VcehqeZJywyhgIO1jyJSN9WEOV8XDh7JyzNfHPfrMX4VaeZnsJAZK1Ww+jWZXkaGYOfu0O67i2EEEJNgh2Ru0WGwaKeicc6m/jVjwuWNNstomJNLxiY1NqTd9Nd3jfdqlKikBPt/7crzbxNynjS5APPVPMMalQi3fcWQghhTIIdkTs9vQsrBsd3XUX/1/JSth10+QlszNvCEZ2OYCcj2lb24fKDtFuLhBBCZA8JdkTu8uwRfF0BYp8nptm5QucfoUwrs90m6eyr2DjzrVYzu2d1rHVarLTG6/KMbZfyFhVCCCEsR4IdkTs8fwIzKkDMM3V6ow+h0WgwETykZv6eqzx+Fs27zUsTFRvHkkM3aViqIEU9HAGIS7IgYKzefC07CYsP2phYhLBZ2dS7q4QQQliGBDsiZykK7Pse1n+kTnf2hbf3gH2BTBX76arTADQv78Vve6+y5NBNdFoNl79oA6j3v3pn0bFM3cMUayvtf/+b2NpBZosLIUSOkGBH5Jyn9+Cr0sbpH1wBR49MF5t0G4crD5+x5NBNILE15+/DN7kbFpnp8lNj/d8UclPdWBrZyUoIIXKEBDsi+z1/Alu/gIM/J6ZZO8KIk1kKchLEJgl2boc8V51TFMWw/YIlJKyXo5jYtcpKJ8GOEELkBAl2RPbRx8GMchB+LzHNyg7qvwuNR5vtNl9tPG/4+st/z6nOpWeaeUaV93HhzH9r6iQENO4ONjjZWqHRQLeafkTF6vF1szf7vYUQQqRNgh1heXExsHwgnPpbnV61J7T7GqxsM120Xq+g0ai3T/hh+5UU8zf7anum75WSQY1LMPzPowDo/uu+stJpOfRJEAB21rLysRBC5CQJdoRlBe+A+e2N00ecAje/DBd39PoTRi45zsjmpbGz1jFh9Wl8XO1YMjAQjUbDpfvGm3gmdStZt1ZGaDSJO48nlXRLh6RfS5AjhBC5gwQ7wjLOr4M/uxmnD9oN3hUzVeTp26GGPayG/deSAnDzyXMiouNwtLXitZ/2Z6rs9Aie3JamX23jygP19PjKRVwNX6e0x5UQQoick7HFS4RIy4UNMN5VHehYO0KLifBpSKYDHTAef5PU+FWnURSF+0+jMl1+uiRr2dk5qgmeLondcDIIWQghch9p2RFZoyhw7xTs/R5OLgV9so0yX3oHmn9mllulttLx0sM3aZINi/bN6FaVTt/tBuCVGkXwc3cgJi5x0LNWFtMRQohcR4IdkXmRofBbJ7h9xPhcl5+gXAewtjPb7dJa6XjB/mtmuxfAR23K8sXa+Nak4c1KAVDVz40rX7QhKlaPnXV8w2jScTreruZ7XiGEEOYhwY7IuLA78F0diApVp7eYCGXbQoEAiywXfOtJ6oOLd196ZNb79W9QHA9HWxxtdbSq6GNI12o12NvoVMd7RjclNk7ByVZ+pIQQIreR38wi/WKew+p34MRidXrjMVB/JFjZWOS2T55F42Jvze1Qy6x6nBKNRkPXGkXSlVfW0BFCiNwrRwco79ixg/bt2+Pr64tGo2HFihWGczExMXz44YdUqlQJR0dHfH196dWrF7dv31aV8fjxY3r27ImLiwtubm707duX8PDwbH6SfO72UVjYDSZ5Gwc6o4LjFwS0UKBz9k4Y1T7fyFvzDlqkfICSnk5GaT1qZ3xavBBCiNwpR4OdZ8+eUaVKFb777jujcxERERw5coSxY8dy5MgRli1bxvnz5+nQoYMqX8+ePTl9+jQbN25kzZo17NixgwEDBmTXI+RvEY/h3w/hx8ZwYV1ievXeMPo6jA8FB3eL3FqvV7h0P5zf98WPw9l+4YFF7gOY3LHqnWYm9uwSQgiRJ2kUxdQyadlPo9GwfPlyOnXqlGKegwcPUrt2ba5du4a/vz9nz56lfPnyHDx4kJo1awKwbt062rRpw82bN/H19U3XvcPCwnB1dSU0NBQXFxdzPE7eFhMZ3111ZgXEJuk68qwAL/8KnmXNdqtVx29zMPgx4ztUUK1RM2PjBWZtvmi2+6SmtJcTF+7FtwZ+37M6TyNj6FbLP1vuLYQQIvPS+/mdp8bshIaGotFocHNzA2Dv3r24ubkZAh2AoKAgtFot+/fvp3PnzibLiYqKIioqcT2WsLAwi9Y7z1AU2PMtbBybmFagWPzU8YBGYO9m9lsmbLNQo2gBOlUrbEjPjkBnZPPSlPV2Vq3f06aSTypXCCGEyIvyTLATGRnJhx9+SI8ePQzR2927d/H0VK+tYmVlhbu7O3fv3k2xrMmTJzNhwgSL1jdPef4Etk6GAz+o00s0g64/W6yrKqmH4RZeDDCZM5+1xMEm/tt/wuoz2XpvIYQQ2StPBDsxMTG8+uqrKIrC7Nmzs1zemDFjGDlypOE4LCwMP78XcEBqZBjsmBrfmpOUnRu8td6s3VW5iYONzhDoAETHmX8ndCGEELlHrg92EgKda9eusWXLFlWfnLe3N/fv31flj42N5fHjx3h7e6dYpq2tLba2md9pO88LvQWHfoWd09XpxRpAy0ngUyVn6mVmDUoVZOfFh0bpP/euqTqOjpVgRwgh8rNcvTdWQqBz8eJFNm3ahIeHh+p8YGAgISEhHD582JC2ZcsW9Ho9derUye7q5n63DsPPQfB1eXWgU28YfHgV+qzJsUDn5pPnrDx2i7tmWkunUmFXfnijhslzRdwcVMcVfOMDaFurXP3jIIQQIpNytGUnPDycS5cuGY6Dg4M5duwY7u7u+Pj48PLLL3PkyBHWrFlDXFycYRyOu7s7NjY2lCtXjlatWtG/f3/mzJlDTEwMQ4cOpXv37umeiZXvxUbBls/h1HIIu5mY7uwTv9px7YFQyDLTrDeducf32y7x1atVCSjoqDp3/VEEd0ITV0Set+cq8/aAu6MNR8Y2z/K9X6/rr+qqSs2MV6vy3dZLvBFYNMv3FUIIkfvkaLBz6NAhmjRpYjhOGEfTu3dvxo8fz6pVqwCoWrWq6rqtW7fSuHFjABYsWMDQoUNp1qwZWq2Wrl27MmvWrGypf672/AmsGg5nV6nTtVbQegrU7Jvqlg57Lj/ki7VnmdSpElX83DJVhX6/HQLgvSXHWPb2S6pzDadtNXnN42fRmbpXcrH69K+o4O1qx+edMr8buxBCiNwtR4Odxo0bk9oyP+lZAsjd3Z2FCxeas1p525mVcHYNnF4G+lj1uT7/QNGX0rVv1Ws/7Qfg9V/2c3J8yyxVKSQicSf0RQeu89Hyk6nmX3TgepbuBxD6PCbFc7bW0l0lhBAvklw/QFmk050TMK8tRCVbM0ijhX6bwLd6pjbnfBoZm3amDBi9LPVAJ7150vLERAvRBy3LEBUTh5eL7EwuhBAvEgl28jK9Hi5vhlN/x+9ZpSSZVdT5ByjXHmwcU77egp5HxyUemH8D9BS9XKMIq47f5vW68eNvhjctyawtl+hey48hTUpmX0WEEELkGhLs5EURj2H/D3DwZ4hINrX6lXlQwfTK0Zbw+75rHLr6mK9eqYKVLrF76OStUMPXGtLXJWkO016uzMROFbGz1gHwTlBpgsp7Uc5HtgERQogXlQQ7edGd47D9y/ivNbr4Fpy6g8G/brZXZeyKUwA0KeNJTJyeaevP80m78rg7JO6CrlcgYMzaLN3H1kqLAnzavjwfLz9lMk9QOS80Go0h0AHQaTVULuKWpXsLIYTI2yTYyYuKN4aqPcGvDpTvAPYFsu3WcXqFmDi9KqAAeBIRbdh2YfifRxnYsLjhXPDDZ1m+b1B5L77pVhVrnVYV7LSr7MOaE3cAmNWjapbvI4QQIv+RYCcv0mig0/c5cusus/dw9k4YR8Y2Z/HBG4b0uGRTvX/YccWs9x3dqizWOuNZVBM6VMBGp6VbLb90r6sjhBDixSKfDiJNXWfvoZCTLXPeqMHxGyEA7Lv8iM/XJG6gGRNnuTE5Tct64ufuYPKch5MtM7pVtdi9hRBC5H0S7Ig0Hb72xCgtLtmA4ynrzlnk3t1q+vFeC8us8CyEEOLFIMGOSLekM6r0GVihOCumvFw5W+4jhBAi/5KlZF9wkTFxaWf6T9LGnOQtO5nxSdty6UpLLrB4/IawjcsUynIdhBBC5H8S7LzATt8OpezYdYxbaXoqd3JJA5zkA5Izo0V5b9XxwEbF6degOJ7OtgC42lubvO77ntWZ2KkiM7tVy3IdhBBC5H8S7LzAZm66CMBve6+lK3/SAOfQVeNxPBnhYKPD30M96HhM6/hWnc86VqCKnxtLBgaavLaAow2v1y2Kq4PpYEgIIYRISsbsvMC0GdwrKyLJFhC/70tfgGRKpcKufP3fDKqXSnqw+9IjrLSJdWlV0YdWFX0yXb4QQgiRlLTsvMC0GXz3a0zcaJb7zupRjZKeTgDM7F6NTlV9WTQg+1d/FkII8WKQlp0XmCaDLTvm2t6qsJu94euCTrZ8013G3gghhLAcCXZeYLoMBjtZsWZYfTxdbLHRabGxkgZFIYQQ2UeCnRdQwkBjbQqxzrbz981+z4qFXc1ephBCCJEeEuy8YPR6hdYzd6DTainr7Wx0/k7oc/rMPZgDNRNCCCEsQ4KdF0hoRAwRMbFcuBcOwNk7YUZ5bodEZne1hBBCCIuSYOcF8fveq4xdeZo+9YqZPB8ZE8fp26GERMRkb8WEEEIIC5Ng5wUxduVpAObtuWry/PA/j7LhzD3K+bhkY62EEEIIy5NpMflA6PMY4vQKMXF69l95RFRs+ve7SrDhzD3AdNdWZui0GooXdDRLWUIIIURWSMtOHnfjcQQNpm6lip8blQu78vu+a7xcowjTX6mSo/VqWKogVx4+y9E6CCGEECAtO3ne6hO3ATh+I8SwhcNfh2/mZJWws9Yy49WqVPcvAKQ8xV0IIYTIDtKyk091nb2HOgHudK5WmFJexlPMLenc560BGN++AkUK2NOxauFsvb8QQgiRlAQ7+dTha084fO0J32+7zM5RTbLtvklXR3Z1sOa9FmWy7d5CCCGEKdKNlYddffiMf07cSTPfqVuhmSrfKo3+p78H12PvmKa0q5y4Q7n0WAkhhMhtJNjJwxpP38bp22nPnlqw/3qmxs1Y6TQ0KFUwxfM1ihbAx9We/71W3ZCWjdttCSGEEOkiwc4LYNelh+gzsWO5VqPht7dqq9LS2sRTI207QgghchkJdkSKtBoNmgw21UjLjhBCiNxGgh2RIkdbnXFiGi1EEusIIYTIbSTYESlysbM2Sivl5ZTqNRltCRJCCCEsTaae5xHhUbHYW+vQZeMKfZWLuKmOx7UrT/PyXkzfcJ7+DYqbvEZCHSGEELmNtOzkcpcfhHPp/lMqfrqenj/vy5Z7/jUokB61/RnXrjwAQeU8sdJq6FDVFz93B2Z2r0bFwq6mL5ZoRwghRC4jLTu52KzNF5mx8YLheN+Vxxy5/gQvFzsKu9mb5R6dqxVm+dFbhuOfetWkZjF3ahZzV6VFx+mxtTIxhicZiXWEEELkNtKyk4slDXQSdPl+Dy99uQVFycRc8mQmdKjAtJcr83Ovmoa0ZmU9jfJpNJo0A532VXwBGNKkZJbrJYQQQpiTtOzkUt9tvZTq+Zi4rAU7V79sa/i6WTlPWlbwwt3RBm0mxwTNeLUKgxoVp7yPS5bqJYQQQpibBDu50P2wSKatP59qnufRcZkuf36yhQI1Gg0/vFEzhdzpY63TUsE3hXE8QgghRA6Sbqwccv1RBH8euE5MnN7oXOjzmDSvD/p6e4bu52iT2A3VoGTKW0AIIYQQ+Y207OSQhtO2AhASEcPgxiUM6bFxelanY3PPB0+jMnS/wgXsaVXRBxc7q0x3VQkhhBB5kQQ7OWzvlUcMblwCRVHYe+URr/203yL3cbS1YmTz0hYpWwghhMjNJNjJYXq9QkycnnazdnH+3lOL3adbTT+LlS2EEELkZjJmJ5sduf6Eu6GRhuM4vcKJm6EWDXTsrLW8KsGOEEKIF5S07GSj07dD6fL9HlVanKJgo7NszBlY3EPG6QghhHhhScuOhd0OeU6n73az8tgtjlx7YnT+QPBj2v9vl0Xu/VOvmgQW92BS50oWKV8IIYTIC6Rlx4L0eoV6X24B4J1Fx5jUuWK23buCrwvNy3vRvLxXtt1TCCGEyI0k2LGgh+Hq6eFajeW6kmZ2r0pBJ1sePYvG3cGGSkVkgT8hhBACJNixqOhkCwbqLBjsALwkiwUKIYQQRmTMjgVFxaqDHXPEOk1NbNQphBBCiJRJsGNByfevymo31tSulfF2tTMcF3CwzlJ5QgghxItAgh0Leh6jDnbm7gnOcpntKvsAYG+tY8/oZoZ0J1vpkRRCCCFMkU9ICwp++Ex1fOpWWNYK1EC9EgVZO7wBfu722NvoGNeuPCdvhdK4jHRvCSGEEKbkaMvOjh07aN++Pb6+vmg0GlasWKE6rygK48aNw8fHB3t7e4KCgrh48aIqz+PHj+nZsycuLi64ubnRt29fwsPDs/EpUjbqrxNmLS+hE6y8rwvOdvFdWG/VD+DrblXRyaKBQgghhEk5Guw8e/aMKlWq8N1335k8P3XqVGbNmsWcOXPYv38/jo6OtGzZksjIxO0WevbsyenTp9m4cSNr1qxhx44dDBgwILsewaKalCmkOtZYeDaXEEIIkR/laDdW69atad26tclziqLwzTff8Mknn9CxY0cAfvvtN7y8vFixYgXdu3fn7NmzrFu3joMHD1KzZk0Avv32W9q0acP06dPx9fXNtmcxl1KeTizoV4dfdgfTuqIPW88/MJyTUEcIIYTIuFw7QDk4OJi7d+8SFBRkSHN1daVOnTrs3bsXgL179+Lm5mYIdACCgoLQarXs378/xbKjoqIICwtT/ctNPF3sGNO6HEXdHXK6KkIIIUSel2uDnbt37wLg5aXe7sDLy8tw7u7du3h6qgfmWllZ4e7ubshjyuTJk3F1dTX88/OzzI7gFXxdsnR98qnqzSvI1g9CCCFERuXaYMeSxowZQ2hoqOHfjRs3LHKfOa/XyNL1miTvzrb3G+NiJ+vqCCGEEBmVa4Mdb29vAO7du6dKv3fvnuGct7c39+/fV52PjY3l8ePHhjym2Nra4uLiovpnCX7uDlz9si3TX6mS5bKsrXLtWyWEEELkarn2EzQgIABvb282b95sSAsLC2P//v0EBgYCEBgYSEhICIcPHzbk2bJlC3q9njp16mR7nVPyco0iJtNrFStglNa5emFLV0cIIYR4oeTobKzw8HAuXbpkOA4ODubYsWO4u7vj7+/PiBEjmDhxIqVKlSIgIICxY8fi6+tLp06dAChXrhytWrWif//+zJkzh5iYGIYOHUr37t3zxEwsD0dbw9fHP23BsRshvFTCw5CWdONQa1lHRwghhMiUHA12Dh06RJMmTQzHI0eOBKB3797MmzePUaNG8ezZMwYMGEBISAj169dn3bp12Nkl7g+1YMEChg4dSrNmzdBqtXTt2pVZs2Zl+7NkVPdaftwJTVwvyNXemkal1evqONpa0a9+ANFxejxd7JIXIYQQQoh00CiKouR0JXJaWFgYrq6uhIaGWmz8TrHR/xi+HtiwOO+1KMOvu4P58t9z2FhpuTDR9HpDQgghhDAtvZ/fsjdWNivv48KYNuUAeOulANwdbKhX0iONq4QQQgiRWRLsZDMrXeLYGxsrLa/WsswaP0IIIYSIl2tnY+VXxQs65nQVhBBCiBeKBDvZ5MNWZSleyJGP2pbL6aoIIYQQLxQZoEz2DFAWQgghhHml9/NbWnaEEEIIka9JsCOEEEKIfE2CHSGEEELkaxLsCCGEECJfk2BHCCGEEPmaBDtCCCGEyNck2BFCCCFEvibBjhBCCCHyNQl2hBBCCJGvSbAjhBBCiHxNgh0hhBBC5GsS7AghhBAiX5NgRwghhBD5mgQ7QgghhMjXrHK6ArmBoihA/FbxQgghhMgbEj63Ez7HUyLBDvD06VMA/Pz8crgmQgghhMiop0+f4urqmuJ5jZJWOPQC0Ov13L59G2dnZzQajdnKDQsLw8/Pjxs3buDi4mK2cnOT/P6M8nx5X35/xvz+fJD/n1GeL/MUReHp06f4+vqi1aY8MkdadgCtVkuRIkUsVr6Li0u+/AZOKr8/ozxf3pffnzG/Px/k/2eU58uc1Fp0EsgAZSGEEELkaxLsCCGEECJfk2DHgmxtbfn000+xtbXN6apYTH5/Rnm+vC+/P2N+fz7I/88oz2d5MkBZCCGEEPmatOwIIYQQIl+TYEcIIYQQ+ZoEO0IIIYTI1yTYEUIIIUS+JsGOBX333XcUK1YMOzs76tSpw4EDB3K6SukyefJkatWqhbOzM56ennTq1Inz58+r8jRu3BiNRqP6N2jQIFWe69ev07ZtWxwcHPD09OSDDz4gNjY2Ox/FpPHjxxvVvWzZsobzkZGRDBkyBA8PD5ycnOjatSv37t1TlZFbnw2gWLFiRs+n0WgYMmQIkDffux07dtC+fXt8fX3RaDSsWLFCdV5RFMaNG4ePjw/29vYEBQVx8eJFVZ7Hjx/Ts2dPXFxccHNzo2/fvoSHh6vynDhxggYNGmBnZ4efnx9Tp0619KMBqT9fTEwMH374IZUqVcLR0RFfX1969erF7du3VWWYet+//PJLVZ6cej5I+z3s06ePUf1btWqlypNX30PA5M+kRqNh2rRphjy5+T1Mz+eCuX53btu2jerVq2Nra0vJkiWZN29e1h9AERaxaNEixcbGRvn111+V06dPK/3791fc3NyUe/fu5XTV0tSyZUtl7ty5yqlTp5Rjx44pbdq0Ufz9/ZXw8HBDnkaNGin9+/dX7ty5Y/gXGhpqOB8bG6tUrFhRCQoKUo4ePaqsXbtWKViwoDJmzJiceCSVTz/9VKlQoYKq7g8ePDCcHzRokOLn56ds3rxZOXTokFK3bl2lXr16hvO5+dkURVHu37+veraNGzcqgLJ161ZFUfLme7d27Vrl448/VpYtW6YAyvLly1Xnv/zyS8XV1VVZsWKFcvz4caVDhw5KQECA8vz5c0OeVq1aKVWqVFH27dun7Ny5UylZsqTSo0cPw/nQ0FDFy8tL6dmzp3Lq1Cnlzz//VOzt7ZUffvghR58vJCRECQoKUhYvXqycO3dO2bt3r1K7dm2lRo0aqjKKFi2qfPbZZ6r3NenPbE4+X1rPqCiK0rt3b6VVq1aq+j9+/FiVJ6++h4qiqJ7rzp07yq+//qpoNBrl8uXLhjy5+T1Mz+eCOX53XrlyRXFwcFBGjhypnDlzRvn2228VnU6nrFu3Lkv1l2DHQmrXrq0MGTLEcBwXF6f8v737j6mq/v8A/rwglx9DuMCFe0EHASJWAgGtu9sPasIg1opySyJHSqWN1GSZMVrp6g+luelaP1hr/tpsWa0fbpU6EajUGwqBRNpN7hBW48fELmBoIPf1/ePzvefTkQuUgvfH5/nY2C7v8z7nvl97cc/7dTnnfW9cXJxs3brVjaO6Pv39/QJAvv32W6Xt/vvvl/Xr10+6zzfffCN+fn7S29urtNXU1EhYWJj89ddfszncaW3evFkyMjJcbrPb7RIQECCffvqp0nb27FkBIBaLRUQ8OzZX1q9fL8nJyeJwOETEu3MnIhMmEofDIUajUbZt26a02e12CQwMlI8++khERM6cOSMA5NSpU0qfgwcPikajkd9//11ERN577z2JiIhQxVhZWSmpqamzHJGaq4nyWidPnhQA0tXVpbQlJCTIjh07Jt3HU+ITcR3jihUrpKioaNJ9fC2HRUVFsmTJElWbN+Xw2nlhps6dL7/8stx+++2q5youLpaCgoIbGi8vY82C0dFRNDc3Iy8vT2nz8/NDXl4eLBaLG0d2fQYHBwEAkZGRqvYPP/wQer0eixcvRlVVFUZGRpRtFosFaWlpMBgMSltBQQGGhobw888/35yBT+HcuXOIi4tDUlISli9fju7ubgBAc3MzxsbGVLlbtGgR4uPjldx5emx/Nzo6in379uHpp59WfcmtN+fuWp2dnejt7VXlLDw8HCaTSZUznU6HO++8U+mTl5cHPz8/NDY2Kn1ycnKg1WqVPgUFBbBarfjjjz9uUjT/zODgIDQaDXQ6naq9uroaUVFRyMzMxLZt21SXB7whvoaGBsTExCA1NRXl5eUYGBhQtvlSDvv6+vD111/jmWeembDNW3J47bwwU+dOi8WiOoazz43Onfwi0Flw4cIFjI+PqxIKAAaDAb/88oubRnV9HA4HKioqcM8992Dx4sVK+5NPPomEhATExcWhra0NlZWVsFqt+PzzzwEAvb29LuN3bnMnk8mEPXv2IDU1FT09PXj99ddx3333ob29Hb29vdBqtRMmEYPBoIzbk2O71pdffgm73Y6VK1cqbd6cO1ecY3I15r/nLCYmRrV9zpw5iIyMVPVJTEyccAzntoiIiFkZ/7915coVVFZWoqSkRPWlii+88AKysrIQGRmJEydOoKqqCj09Pdi+fTsAz4/vwQcfxNKlS5GYmAibzYZXXnkFhYWFsFgs8Pf396kc7t27F3PnzsXSpUtV7d6SQ1fzwkydOyfrMzQ0hMuXLyM4OPi6xsxih6a0Zs0atLe349ixY6r21atXK4/T0tIQGxuL3Nxc2Gw2JCcn3+xh/iuFhYXK4/T0dJhMJiQkJOCTTz657heSp9q5cycKCwsRFxentHlz7v7XjY2NYdmyZRAR1NTUqLa9+OKLyuP09HRotVo899xz2Lp1q1d8DcETTzyhPE5LS0N6ejqSk5PR0NCA3NxcN45s5u3atQvLly9HUFCQqt1bcjjZvODJeBlrFuj1evj7+0+4C72vrw9Go9FNo/r31q5di6+++gr19fWYP3/+lH1NJhMAoKOjAwBgNBpdxu/c5kl0Oh0WLlyIjo4OGI1GjI6Owm63q/r8PXfeEltXVxdqa2vx7LPPTtnPm3MH/HdMU73ejEYj+vv7VduvXr2Kixcvek1enYVOV1cXjhw5ovqvjismkwlXr17F+fPnAXh+fNdKSkqCXq9X/V16ew4B4Pvvv4fVap32dQl4Zg4nmxdm6tw5WZ+wsLAbejPKYmcWaLVaZGdn4+jRo0qbw+HA0aNHYTab3Tiyf0ZEsHbtWnzxxReoq6ub8G9TV1pbWwEAsbGxAACz2YyffvpJdXJynqBvu+22WRn39bp06RJsNhtiY2ORnZ2NgIAAVe6sViu6u7uV3HlLbLt370ZMTAweeuihKft5c+4AIDExEUajUZWzoaEhNDY2qnJmt9vR3Nys9Kmrq4PD4VCKPbPZjO+++w5jY2NKnyNHjiA1NdXtlz+chc65c+dQW1uLqKioafdpbW2Fn5+fcunHk+Nz5bfffsPAwIDq79Kbc+i0c+dOZGdnIyMjY9q+npTD6eaFmTp3ms1m1TGcfW547ryh25tpUvv375fAwEDZs2ePnDlzRlavXi06nU51F7qnKi8vl/DwcGloaFAtgRwZGRERkY6ODnnjjTekqalJOjs75cCBA5KUlCQ5OTnKMZxLDPPz86W1tVUOHTok0dHRHrE8e8OGDdLQ0CCdnZ1y/PhxycvLE71eL/39/SLyn+WT8fHxUldXJ01NTWI2m8VsNiv7e3JsTuPj4xIfHy+VlZWqdm/N3fDwsLS0tEhLS4sAkO3bt0tLS4uyGqm6ulp0Op0cOHBA2trapKioyOXS88zMTGlsbJRjx45JSkqKatmy3W4Xg8EgpaWl0t7eLvv375eQkJCbsqx3qvhGR0flkUcekfnz50tra6vqNelcwXLixAnZsWOHtLa2is1mk3379kl0dLQ89dRTHhHfdDEODw/LSy+9JBaLRTo7O6W2tlaysrIkJSVFrly5ohzDW3PoNDg4KCEhIVJTUzNhf0/P4XTzgsjMnDudS883btwoZ8+elXfffZdLzz3d22+/LfHx8aLVauWuu+6SH374wd1D+kcAuPzZvXu3iIh0d3dLTk6OREZGSmBgoCxYsEA2btyo+qwWEZHz589LYWGhBAcHi16vlw0bNsjY2JgbIlIrLi6W2NhY0Wq1Mm/ePCkuLpaOjg5l++XLl+X555+XiIgICQkJkccee0x6enpUx/DU2JwOHz4sAMRqtaravTV39fX1Lv8mV6xYISL/WX7+2muvicFgkMDAQMnNzZ0Q+8DAgJSUlEhoaKiEhYVJWVmZDA8Pq/qcPn1a7r33XgkMDJR58+ZJdXW12+Pr7Oyc9DXp/Oyk5uZmMZlMEh4eLkFBQXLrrbfKli1bVIWCO+ObLsaRkRHJz8+X6OhoCQgIkISEBFm1atWEN4femkOn999/X4KDg8Vut0/Y39NzON28IDJz5876+nq54447RKvVSlJSkuo5rpfm/4MgIiIi8km8Z4eIiIh8GosdIiIi8mksdoiIiMinsdghIiIin8Zih4iIiHwaix0iIiLyaSx2iIiIyKex2CEir7dy5Uo8+uij7h4GEXkofus5EXk0jUYz5fbNmzfjrbfeAj8flYgmw2KHiDxaT0+P8vjjjz/Gpk2bYLValbbQ0FCEhoa6Y2hE5CV4GYuIPJrRaFR+wsPDodFoVG2hoaETLmM98MADWLduHSoqKhAREQGDwYAPPvgAf/75J8rKyjB37lwsWLAABw8eVD1Xe3s7CgsLERoaCoPBgNLSUly4cOEmR0xEM43FDhH5pL1790Kv1+PkyZNYt24dysvL8fjjj+Puu+/Gjz/+iPz8fJSWlmJkZAQAYLfbsWTJEmRmZqKpqQmHDh1CX18fli1b5uZIiOhGsdghIp+UkZGBV199FSkpKaiqqkJQUBD0ej1WrVqFlJQUbNq0CQMDA2hrawMAvPPOO8jMzMSWLVuwaNEiZGZmYteuXaivr8evv/7q5miI6Ebwnh0i8knp6enKY39/f0RFRSEtLU1pMxgMAID+/n4AwOnTp1FfX+/y/h+bzYaFCxfO8oiJaLaw2CEinxQQEKD6XaPRqNqcq7wcDgcA4NKlS3j44Yfx5ptvTjhWbGzsLI6UiGYbix0iIgBZWVn47LPPcMstt2DOHJ4aiXwJ79khIgKwZs0aXLx4ESUlJTh16hRsNhsOHz6MsrIyjI+Pu3t4RHQDWOwQEQGIi4vD8ePHMT4+jvz8fKSlpaGiogI6nQ5+fjxVEnkzjfBjR4mIiMiH8e0KERER+TQWO0REROTTWOwQERGRT2OxQ0RERD6NxQ4RERH5NBY7RERE5NNY7BAREZFPY7FDREREPo3FDhEREfk0FjtERETk01jsEBERkU9jsUNEREQ+7f8AiDZCePMTbCcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Prepare true values for comparison\n",
    "true_values = scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs true values\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(true_values, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.title('Predictions vs True Data (Both Scaled Back)')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 16.4399 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 2.4443   \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 1.7956 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 1.2787 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.7620 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.3238 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1402 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0654\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0479 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0399   \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0422 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0305\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0258 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0241 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0293 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0230 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0220 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0190 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0205 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0159\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 298ms/step - loss: 0.0025\n",
      "Test loss: 0.0028562627267092466\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "## Write your code here.\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout = Dropout(0.5)(flatten)\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 609ms/step - loss: 0.0339 \n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 604ms/step - loss: 0.0453 \n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 610ms/step - loss: 0.0269 \n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 608ms/step - loss: 0.0259 \n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 602ms/step - loss: 0.0298 \n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 592ms/step - loss: 0.0208 \n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 595ms/step - loss: 0.0237 \n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 597ms/step - loss: 0.0259 \n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 599ms/step - loss: 0.0197 \n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 599ms/step - loss: 0.0191 \n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 605ms/step - loss: 0.0164 \n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 605ms/step - loss: 0.0189 \n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 603ms/step - loss: 0.0163 \n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 602ms/step - loss: 0.0145 \n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 604ms/step - loss: 0.0178 \n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 607ms/step - loss: 0.0176 \n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 606ms/step - loss: 0.0119 \n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 611ms/step - loss: 0.0167 \n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 607ms/step - loss: 0.0109 \n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 607ms/step - loss: 0.0164 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 302ms/step - loss: 0.0020\n",
      "Test loss: 0.001349122729152441\n",
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0115\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0063\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0057\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0049\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0056\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0046\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0044\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0039\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0040\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0047\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0037\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0039\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0044\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0043\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0038\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0043\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0037\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0040 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 300ms/step - loss: 3.6335e-04\n",
      "Test loss: 0.00040041134343482554\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 0.1617 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0211 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0072 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0050\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0084   \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0047\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0044 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0031 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0052\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0037   \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0037 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0022 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0026\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0023 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0043 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0036 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0027 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0020 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0023\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0029 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 303ms/step - loss: 0.0011    \n",
      "Test loss with tanh activation: 0.0012179246405139565\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8aae4de69f29de06e63c5f2d04ef24811d42d1553c8ac316f7ad75d55f2c2d79"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
